{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_train = np.arange(11,111, dtype='float64') + np.random.normal(scale=2, size=100)\n",
    "trend_test_outofrange = np.arange(111,131, dtype='float64') + np.random.normal(scale=2, size=20)\n",
    "trend_test_inrange = np.arange(31,51, dtype='float64') + np.random.normal(scale=2, size=20)\n",
    "\n",
    "train = trend_train\n",
    "test = trend_test_outofrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_stateful = False\n",
    "\n",
    "lstm_num_timesteps = 4 # for diffed series\n",
    "lstm_num_predictions = lstm_num_timesteps\n",
    "effective_timesteps = lstm_num_timesteps+1\n",
    "lstm_num_features = 1\n",
    "\n",
    "num_neurons = 4\n",
    "batchsize = 1\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return(diff)\n",
    "\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    # history[-len(series)] is first item, history[-len(series) - 1] is second, etc. \n",
    "    return yhat + history[-interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27338302596317021,\n",
       " 1.7310265497711441,\n",
       " 1.7451128164638021,\n",
       " 0.29727845971444822,\n",
       " -0.061718680499211587,\n",
       " 2.0725813598952101,\n",
       " -3.0916986883747199,\n",
       " 3.3959042561117627,\n",
       " 5.4305546542064178,\n",
       " 0.16910568625694111,\n",
       " 0.90280661853361721,\n",
       " -0.65152964591959517,\n",
       " 1.5494066806911633,\n",
       " -0.63092790701740853,\n",
       " 2.8014413479762936,\n",
       " 1.8094951282746763,\n",
       " -0.45663481451620669,\n",
       " 2.7467181214053369,\n",
       " -2.3186887828005354]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_diff = difference(train, 1)\n",
    "test_diff = difference(test, 1)\n",
    "test_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   var1(t-3)  var1(t-2)  var1(t-1)  var1(t)  var1(t+1)  var1(t+2)\n",
      "3        0.0        1.0        2.0        3        4.0        5.0\n",
      "4        1.0        2.0        3.0        4        5.0        6.0\n",
      "5        2.0        3.0        4.0        5        6.0        7.0\n",
      "6        3.0        4.0        5.0        6        7.0        8.0\n",
      "7        4.0        5.0        6.0        7        8.0        9.0\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "    data: Sequence of observations as a list or NumPy array.\n",
    "    n_in: Number of lag observations as input (X).\n",
    "    n_out: Number of observations as output (y).\n",
    "    dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "    Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "values = [x for x in range(10)]\n",
    "data = series_to_supervised(values,3,3)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-4)</th>\n",
       "      <th>var1(t-3)</th>\n",
       "      <th>var1(t-2)</th>\n",
       "      <th>var1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var1(t+1)</th>\n",
       "      <th>var1(t+2)</th>\n",
       "      <th>var1(t+3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.273383</td>\n",
       "      <td>1.731027</td>\n",
       "      <td>1.745113</td>\n",
       "      <td>0.297278</td>\n",
       "      <td>-0.061719</td>\n",
       "      <td>2.072581</td>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.731027</td>\n",
       "      <td>1.745113</td>\n",
       "      <td>0.297278</td>\n",
       "      <td>-0.061719</td>\n",
       "      <td>2.072581</td>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.745113</td>\n",
       "      <td>0.297278</td>\n",
       "      <td>-0.061719</td>\n",
       "      <td>2.072581</td>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.297278</td>\n",
       "      <td>-0.061719</td>\n",
       "      <td>2.072581</td>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.061719</td>\n",
       "      <td>2.072581</td>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.072581</td>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-3.091699</td>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "      <td>-0.630928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.395904</td>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "      <td>-0.630928</td>\n",
       "      <td>2.801441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.430555</td>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "      <td>-0.630928</td>\n",
       "      <td>2.801441</td>\n",
       "      <td>1.809495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.169106</td>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "      <td>-0.630928</td>\n",
       "      <td>2.801441</td>\n",
       "      <td>1.809495</td>\n",
       "      <td>-0.456635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.902807</td>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "      <td>-0.630928</td>\n",
       "      <td>2.801441</td>\n",
       "      <td>1.809495</td>\n",
       "      <td>-0.456635</td>\n",
       "      <td>2.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.651530</td>\n",
       "      <td>1.549407</td>\n",
       "      <td>-0.630928</td>\n",
       "      <td>2.801441</td>\n",
       "      <td>1.809495</td>\n",
       "      <td>-0.456635</td>\n",
       "      <td>2.746718</td>\n",
       "      <td>-2.318689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    var1(t-4)  var1(t-3)  var1(t-2)  var1(t-1)   var1(t)  var1(t+1)  \\\n",
       "4    0.273383   1.731027   1.745113   0.297278 -0.061719   2.072581   \n",
       "5    1.731027   1.745113   0.297278  -0.061719  2.072581  -3.091699   \n",
       "6    1.745113   0.297278  -0.061719   2.072581 -3.091699   3.395904   \n",
       "7    0.297278  -0.061719   2.072581  -3.091699  3.395904   5.430555   \n",
       "8   -0.061719   2.072581  -3.091699   3.395904  5.430555   0.169106   \n",
       "9    2.072581  -3.091699   3.395904   5.430555  0.169106   0.902807   \n",
       "10  -3.091699   3.395904   5.430555   0.169106  0.902807  -0.651530   \n",
       "11   3.395904   5.430555   0.169106   0.902807 -0.651530   1.549407   \n",
       "12   5.430555   0.169106   0.902807  -0.651530  1.549407  -0.630928   \n",
       "13   0.169106   0.902807  -0.651530   1.549407 -0.630928   2.801441   \n",
       "14   0.902807  -0.651530   1.549407  -0.630928  2.801441   1.809495   \n",
       "15  -0.651530   1.549407  -0.630928   2.801441  1.809495  -0.456635   \n",
       "\n",
       "    var1(t+2)  var1(t+3)  \n",
       "4   -3.091699   3.395904  \n",
       "5    3.395904   5.430555  \n",
       "6    5.430555   0.169106  \n",
       "7    0.169106   0.902807  \n",
       "8    0.902807  -0.651530  \n",
       "9   -0.651530   1.549407  \n",
       "10   1.549407  -0.630928  \n",
       "11  -0.630928   2.801441  \n",
       "12   2.801441   1.809495  \n",
       "13   1.809495  -0.456635  \n",
       "14  -0.456635   2.746718  \n",
       "15   2.746718  -2.318689  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_diff_df = series_to_supervised(train_diff,lstm_num_timesteps, lstm_num_predictions)\n",
    "test_diff_df = series_to_supervised(test_diff,lstm_num_timesteps, lstm_num_predictions)\n",
    "test_diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_diff_matrix = train_diff_df.values\n",
    "test_diff_matrix = test_diff_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27338303,  1.73102655,  1.74511282,  0.29727846, -0.06171868,\n",
       "         2.07258136, -3.09169869,  3.39590426],\n",
       "       [ 1.73102655,  1.74511282,  0.29727846, -0.06171868,  2.07258136,\n",
       "        -3.09169869,  3.39590426,  5.43055465],\n",
       "       [ 1.74511282,  0.29727846, -0.06171868,  2.07258136, -3.09169869,\n",
       "         3.39590426,  5.43055465,  0.16910569],\n",
       "       [ 0.29727846, -0.06171868,  2.07258136, -3.09169869,  3.39590426,\n",
       "         5.43055465,  0.16910569,  0.90280662],\n",
       "       [-0.06171868,  2.07258136, -3.09169869,  3.39590426,  5.43055465,\n",
       "         0.16910569,  0.90280662, -0.65152965],\n",
       "       [ 2.07258136, -3.09169869,  3.39590426,  5.43055465,  0.16910569,\n",
       "         0.90280662, -0.65152965,  1.54940668],\n",
       "       [-3.09169869,  3.39590426,  5.43055465,  0.16910569,  0.90280662,\n",
       "        -0.65152965,  1.54940668, -0.63092791],\n",
       "       [ 3.39590426,  5.43055465,  0.16910569,  0.90280662, -0.65152965,\n",
       "         1.54940668, -0.63092791,  2.80144135],\n",
       "       [ 5.43055465,  0.16910569,  0.90280662, -0.65152965,  1.54940668,\n",
       "        -0.63092791,  2.80144135,  1.80949513],\n",
       "       [ 0.16910569,  0.90280662, -0.65152965,  1.54940668, -0.63092791,\n",
       "         2.80144135,  1.80949513, -0.45663481],\n",
       "       [ 0.90280662, -0.65152965,  1.54940668, -0.63092791,  2.80144135,\n",
       "         1.80949513, -0.45663481,  2.74671812],\n",
       "       [-0.65152965,  1.54940668, -0.63092791,  2.80144135,  1.80949513,\n",
       "        -0.45663481,  2.74671812, -2.31868878]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(train, test):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "def invert_scale(scaler, X, yhat):\n",
    "    new_row = [x for x in X] + [y for y in yhat]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -lstm_num_timesteps:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15835973,  0.34382199,  0.34561424,  0.16140005,  0.11572329,\n",
       "         0.38727948, -0.26979411,  0.55565155],\n",
       "       [ 0.34382199,  0.34561424,  0.16140005,  0.11572329,  0.38727948,\n",
       "        -0.26979411,  0.55565155,  0.81452888],\n",
       "       [ 0.34561424,  0.16140005,  0.11572329,  0.38727948, -0.26979411,\n",
       "         0.55565155,  0.81452888,  0.14509207],\n",
       "       [ 0.16140005,  0.11572329,  0.38727948, -0.26979411,  0.55565155,\n",
       "         0.81452888,  0.14509207,  0.238444  ],\n",
       "       [ 0.11572329,  0.38727948, -0.26979411,  0.55565155,  0.81452888,\n",
       "         0.14509207,  0.238444  ,  0.04067911],\n",
       "       [ 0.38727948, -0.26979411,  0.55565155,  0.81452888,  0.14509207,\n",
       "         0.238444  ,  0.04067911,  0.32071371],\n",
       "       [-0.26979411,  0.55565155,  0.81452888,  0.14509207,  0.238444  ,\n",
       "         0.04067911,  0.32071371,  0.04330035],\n",
       "       [ 0.55565155,  0.81452888,  0.14509207,  0.238444  ,  0.04067911,\n",
       "         0.32071371,  0.04330035,  0.48001547],\n",
       "       [ 0.81452888,  0.14509207,  0.238444  ,  0.04067911,  0.32071371,\n",
       "         0.04330035,  0.48001547,  0.35380588],\n",
       "       [ 0.14509207,  0.238444  ,  0.04067911,  0.32071371,  0.04330035,\n",
       "         0.48001547,  0.35380588,  0.06547641],\n",
       "       [ 0.238444  ,  0.04067911,  0.32071371,  0.04330035,  0.48001547,\n",
       "         0.35380588,  0.06547641,  0.4730528 ],\n",
       "       [ 0.04067911,  0.32071371,  0.04330035,  0.48001547,  0.35380588,\n",
       "         0.06547641,  0.4730528 , -0.17144073]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler, train_scaled, test_scaled = scale(train_diff_matrix, test_diff_matrix)\n",
    "test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lstm_stateful(train, batchsize, num_epochs, num_neurons):    \n",
    "    X, y = train[:, 0:lstm_num_timesteps], train[:, lstm_num_timesteps:]\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X = np.reshape(X, (X.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    y = np.reshape(y, (y.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(num_neurons, batch_input_shape=(batchsize, X.shape[1], X.shape[2]),\n",
    "                   stateful=True, return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.add(Activation(\"linear\"))  \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    for i in range(num_epochs):\n",
    "        model.fit(X, y, epochs=1, batch_size=batchsize, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_lstm_stateless(train, batchsize, num_epochs, num_neurons):\n",
    "    X, y = train[:, 0:lstm_num_timesteps], train[:, lstm_num_timesteps:]\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X = np.reshape(X, (X.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "    y = np.reshape(y, (y.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(num_neurons, batch_input_shape=(batchsize, X.shape[1], X.shape[2]), return_sequences = True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.add(Activation(\"linear\"))  \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    model.fit(X, y, epochs=num_epochs, batch_size=batchsize, verbose=1, shuffle=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forecast_lstm(model, batch_size, X):\n",
    "    if lstm_stateful:\n",
    "         model.reset_states()\n",
    "    X = X.reshape(1, len(X), 1)\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,:,0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_diff = difference(train, 1)\n",
    "test_diff = difference(test, 1)\n",
    "train_matrix = series_to_supervised(train_diff, lstm_num_timesteps, lstm_num_predictions).values\n",
    "test_matrix = series_to_supervised(test_diff, lstm_num_timesteps, lstm_num_predictions).values\n",
    "scaler, train_scaled, test_scaled = scale(train_matrix, test_matrix)\n",
    "X, y = train_scaled[:, 0:lstm_num_timesteps], train_scaled[:, lstm_num_timesteps:]\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], lstm_num_timesteps, lstm_num_features))\n",
    "y = np.reshape(y, (y.shape[0], lstm_num_predictions, lstm_num_features))\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, batch_input_shape=(batchsize, X.shape[1], X.shape[2]), return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Activation(\"linear\"))  \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()\n",
    "model.fit(X, y, epochs=1, batch_size=batchsize, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (1, 4, 4)                 96        \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (1, 4, 1)                 5         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (1, 4, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "92/92 [==============================] - 0s - loss: 0.1833     \n",
      "Epoch 2/500\n",
      "92/92 [==============================] - 0s - loss: 0.1542     \n",
      "Epoch 3/500\n",
      "92/92 [==============================] - 0s - loss: 0.1518     \n",
      "Epoch 4/500\n",
      "92/92 [==============================] - 0s - loss: 0.1514     \n",
      "Epoch 5/500\n",
      "92/92 [==============================] - 0s - loss: 0.1511     \n",
      "Epoch 6/500\n",
      "92/92 [==============================] - 0s - loss: 0.1508     \n",
      "Epoch 7/500\n",
      "92/92 [==============================] - 0s - loss: 0.1506     \n",
      "Epoch 8/500\n",
      "92/92 [==============================] - 0s - loss: 0.1502     \n",
      "Epoch 9/500\n",
      "92/92 [==============================] - 0s - loss: 0.1500     \n",
      "Epoch 10/500\n",
      "92/92 [==============================] - 0s - loss: 0.1500     \n",
      "Epoch 11/500\n",
      "92/92 [==============================] - 0s - loss: 0.1497     \n",
      "Epoch 12/500\n",
      "92/92 [==============================] - 0s - loss: 0.1496     \n",
      "Epoch 13/500\n",
      "92/92 [==============================] - 0s - loss: 0.1494     \n",
      "Epoch 14/500\n",
      "92/92 [==============================] - 0s - loss: 0.1493     \n",
      "Epoch 15/500\n",
      "92/92 [==============================] - 0s - loss: 0.1492     \n",
      "Epoch 16/500\n",
      "92/92 [==============================] - 0s - loss: 0.1491     \n",
      "Epoch 17/500\n",
      "92/92 [==============================] - 0s - loss: 0.1490     \n",
      "Epoch 18/500\n",
      "92/92 [==============================] - 0s - loss: 0.1491     \n",
      "Epoch 19/500\n",
      "92/92 [==============================] - 0s - loss: 0.1488     \n",
      "Epoch 20/500\n",
      "92/92 [==============================] - 0s - loss: 0.1487     \n",
      "Epoch 21/500\n",
      "92/92 [==============================] - 0s - loss: 0.1487     \n",
      "Epoch 22/500\n",
      "92/92 [==============================] - 0s - loss: 0.1485     \n",
      "Epoch 23/500\n",
      "92/92 [==============================] - 0s - loss: 0.1485     \n",
      "Epoch 24/500\n",
      "92/92 [==============================] - 0s - loss: 0.1484     \n",
      "Epoch 25/500\n",
      "92/92 [==============================] - 0s - loss: 0.1483     \n",
      "Epoch 26/500\n",
      "92/92 [==============================] - 0s - loss: 0.1483     \n",
      "Epoch 27/500\n",
      "92/92 [==============================] - 0s - loss: 0.1482     \n",
      "Epoch 28/500\n",
      "92/92 [==============================] - 0s - loss: 0.1481     \n",
      "Epoch 29/500\n",
      "92/92 [==============================] - 0s - loss: 0.1480     \n",
      "Epoch 30/500\n",
      "92/92 [==============================] - 0s - loss: 0.1479     \n",
      "Epoch 31/500\n",
      "92/92 [==============================] - 0s - loss: 0.1478     \n",
      "Epoch 32/500\n",
      "92/92 [==============================] - 0s - loss: 0.1477     \n",
      "Epoch 33/500\n",
      "92/92 [==============================] - 0s - loss: 0.1476     \n",
      "Epoch 34/500\n",
      "92/92 [==============================] - 0s - loss: 0.1475     \n",
      "Epoch 35/500\n",
      "92/92 [==============================] - 0s - loss: 0.1474     \n",
      "Epoch 36/500\n",
      "92/92 [==============================] - 0s - loss: 0.1474     \n",
      "Epoch 37/500\n",
      "92/92 [==============================] - 0s - loss: 0.1471     \n",
      "Epoch 38/500\n",
      "92/92 [==============================] - 0s - loss: 0.1471     \n",
      "Epoch 39/500\n",
      "92/92 [==============================] - 0s - loss: 0.1471     \n",
      "Epoch 40/500\n",
      "92/92 [==============================] - 0s - loss: 0.1468     \n",
      "Epoch 41/500\n",
      "92/92 [==============================] - 0s - loss: 0.1468     \n",
      "Epoch 42/500\n",
      "92/92 [==============================] - 0s - loss: 0.1467     \n",
      "Epoch 43/500\n",
      "92/92 [==============================] - 0s - loss: 0.1466     \n",
      "Epoch 44/500\n",
      "92/92 [==============================] - 0s - loss: 0.1465     \n",
      "Epoch 45/500\n",
      "92/92 [==============================] - 0s - loss: 0.1463     \n",
      "Epoch 46/500\n",
      "92/92 [==============================] - 0s - loss: 0.1461     \n",
      "Epoch 47/500\n",
      "92/92 [==============================] - 0s - loss: 0.1460     \n",
      "Epoch 48/500\n",
      "92/92 [==============================] - 0s - loss: 0.1460     \n",
      "Epoch 49/500\n",
      "92/92 [==============================] - 0s - loss: 0.1458     \n",
      "Epoch 50/500\n",
      "92/92 [==============================] - 0s - loss: 0.1457     \n",
      "Epoch 51/500\n",
      "92/92 [==============================] - 0s - loss: 0.1454     \n",
      "Epoch 52/500\n",
      "92/92 [==============================] - 0s - loss: 0.1453     \n",
      "Epoch 53/500\n",
      "92/92 [==============================] - 0s - loss: 0.1452     \n",
      "Epoch 54/500\n",
      "92/92 [==============================] - 0s - loss: 0.1450     \n",
      "Epoch 55/500\n",
      "92/92 [==============================] - 0s - loss: 0.1449     \n",
      "Epoch 56/500\n",
      "92/92 [==============================] - 0s - loss: 0.1447     \n",
      "Epoch 57/500\n",
      "92/92 [==============================] - 0s - loss: 0.1444     \n",
      "Epoch 58/500\n",
      "92/92 [==============================] - 0s - loss: 0.1445     \n",
      "Epoch 59/500\n",
      "92/92 [==============================] - 0s - loss: 0.1442     \n",
      "Epoch 60/500\n",
      "92/92 [==============================] - 0s - loss: 0.1440     \n",
      "Epoch 61/500\n",
      "92/92 [==============================] - 0s - loss: 0.1437     \n",
      "Epoch 62/500\n",
      "92/92 [==============================] - 0s - loss: 0.1436     \n",
      "Epoch 63/500\n",
      "92/92 [==============================] - 0s - loss: 0.1435     \n",
      "Epoch 64/500\n",
      "92/92 [==============================] - 0s - loss: 0.1433     \n",
      "Epoch 65/500\n",
      "92/92 [==============================] - 0s - loss: 0.1431     \n",
      "Epoch 66/500\n",
      "92/92 [==============================] - 0s - loss: 0.1430     \n",
      "Epoch 67/500\n",
      "92/92 [==============================] - 0s - loss: 0.1428     \n",
      "Epoch 68/500\n",
      "92/92 [==============================] - 0s - loss: 0.1429     \n",
      "Epoch 69/500\n",
      "92/92 [==============================] - 0s - loss: 0.1424     \n",
      "Epoch 70/500\n",
      "92/92 [==============================] - 0s - loss: 0.1421     \n",
      "Epoch 71/500\n",
      "92/92 [==============================] - 0s - loss: 0.1422     \n",
      "Epoch 72/500\n",
      "92/92 [==============================] - 0s - loss: 0.1420     \n",
      "Epoch 73/500\n",
      "92/92 [==============================] - 0s - loss: 0.1419     \n",
      "Epoch 74/500\n",
      "92/92 [==============================] - 0s - loss: 0.1418     \n",
      "Epoch 75/500\n",
      "92/92 [==============================] - 0s - loss: 0.1416     \n",
      "Epoch 76/500\n",
      "92/92 [==============================] - 0s - loss: 0.1414     \n",
      "Epoch 77/500\n",
      "92/92 [==============================] - 0s - loss: 0.1412     \n",
      "Epoch 78/500\n",
      "92/92 [==============================] - 0s - loss: 0.1412     \n",
      "Epoch 79/500\n",
      "92/92 [==============================] - 0s - loss: 0.1410     \n",
      "Epoch 80/500\n",
      "92/92 [==============================] - 0s - loss: 0.1411     \n",
      "Epoch 81/500\n",
      "92/92 [==============================] - 0s - loss: 0.1407     \n",
      "Epoch 82/500\n",
      "92/92 [==============================] - 0s - loss: 0.1405     \n",
      "Epoch 83/500\n",
      "92/92 [==============================] - 0s - loss: 0.1403     \n",
      "Epoch 84/500\n",
      "92/92 [==============================] - 0s - loss: 0.1403     \n",
      "Epoch 85/500\n",
      "92/92 [==============================] - 0s - loss: 0.1402     \n",
      "Epoch 86/500\n",
      "92/92 [==============================] - 0s - loss: 0.1399     \n",
      "Epoch 87/500\n",
      "92/92 [==============================] - 0s - loss: 0.1400     \n",
      "Epoch 88/500\n",
      "92/92 [==============================] - 0s - loss: 0.1397     \n",
      "Epoch 89/500\n",
      "92/92 [==============================] - 0s - loss: 0.1395     \n",
      "Epoch 90/500\n",
      "92/92 [==============================] - 0s - loss: 0.1396     \n",
      "Epoch 91/500\n",
      "92/92 [==============================] - 0s - loss: 0.1392     \n",
      "Epoch 92/500\n",
      "92/92 [==============================] - 0s - loss: 0.1392     \n",
      "Epoch 93/500\n",
      "92/92 [==============================] - 0s - loss: 0.1390     \n",
      "Epoch 94/500\n",
      "92/92 [==============================] - 0s - loss: 0.1388     \n",
      "Epoch 95/500\n",
      "92/92 [==============================] - 0s - loss: 0.1386     \n",
      "Epoch 96/500\n",
      "92/92 [==============================] - 0s - loss: 0.1385     \n",
      "Epoch 97/500\n",
      "92/92 [==============================] - 0s - loss: 0.1384     \n",
      "Epoch 98/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s - loss: 0.1383     \n",
      "Epoch 99/500\n",
      "92/92 [==============================] - 0s - loss: 0.1380     \n",
      "Epoch 100/500\n",
      "92/92 [==============================] - 0s - loss: 0.1380     \n",
      "Epoch 101/500\n",
      "92/92 [==============================] - 0s - loss: 0.1379     \n",
      "Epoch 102/500\n",
      "92/92 [==============================] - 0s - loss: 0.1378     \n",
      "Epoch 103/500\n",
      "92/92 [==============================] - 0s - loss: 0.1375     \n",
      "Epoch 104/500\n",
      "92/92 [==============================] - 0s - loss: 0.1374     \n",
      "Epoch 105/500\n",
      "92/92 [==============================] - 0s - loss: 0.1371     \n",
      "Epoch 106/500\n",
      "92/92 [==============================] - 0s - loss: 0.1370     \n",
      "Epoch 107/500\n",
      "92/92 [==============================] - 0s - loss: 0.1371     \n",
      "Epoch 108/500\n",
      "92/92 [==============================] - 0s - loss: 0.1369     \n",
      "Epoch 109/500\n",
      "92/92 [==============================] - 0s - loss: 0.1366     \n",
      "Epoch 110/500\n",
      "92/92 [==============================] - 0s - loss: 0.1364     \n",
      "Epoch 111/500\n",
      "92/92 [==============================] - 0s - loss: 0.1362     \n",
      "Epoch 112/500\n",
      "92/92 [==============================] - 0s - loss: 0.1362     \n",
      "Epoch 113/500\n",
      "92/92 [==============================] - 0s - loss: 0.1360     \n",
      "Epoch 114/500\n",
      "92/92 [==============================] - 0s - loss: 0.1360     \n",
      "Epoch 115/500\n",
      "92/92 [==============================] - 0s - loss: 0.1357     \n",
      "Epoch 116/500\n",
      "92/92 [==============================] - 0s - loss: 0.1358     \n",
      "Epoch 117/500\n",
      "92/92 [==============================] - 0s - loss: 0.1358     \n",
      "Epoch 118/500\n",
      "92/92 [==============================] - 0s - loss: 0.1354     \n",
      "Epoch 119/500\n",
      "92/92 [==============================] - 0s - loss: 0.1357     \n",
      "Epoch 120/500\n",
      "92/92 [==============================] - 0s - loss: 0.1352     \n",
      "Epoch 121/500\n",
      "92/92 [==============================] - 0s - loss: 0.1351     \n",
      "Epoch 122/500\n",
      "92/92 [==============================] - 0s - loss: 0.1349     \n",
      "Epoch 123/500\n",
      "92/92 [==============================] - 0s - loss: 0.1351     \n",
      "Epoch 124/500\n",
      "92/92 [==============================] - 0s - loss: 0.1348     \n",
      "Epoch 125/500\n",
      "92/92 [==============================] - 0s - loss: 0.1348     \n",
      "Epoch 126/500\n",
      "92/92 [==============================] - 0s - loss: 0.1345     \n",
      "Epoch 127/500\n",
      "92/92 [==============================] - 0s - loss: 0.1346     \n",
      "Epoch 128/500\n",
      "92/92 [==============================] - 0s - loss: 0.1346     \n",
      "Epoch 129/500\n",
      "92/92 [==============================] - 0s - loss: 0.1343     \n",
      "Epoch 130/500\n",
      "92/92 [==============================] - 0s - loss: 0.1341     \n",
      "Epoch 131/500\n",
      "92/92 [==============================] - 0s - loss: 0.1343     \n",
      "Epoch 132/500\n",
      "92/92 [==============================] - 0s - loss: 0.1340     \n",
      "Epoch 133/500\n",
      "92/92 [==============================] - 0s - loss: 0.1344     \n",
      "Epoch 134/500\n",
      "92/92 [==============================] - 0s - loss: 0.1341     \n",
      "Epoch 135/500\n",
      "92/92 [==============================] - 0s - loss: 0.1340     \n",
      "Epoch 136/500\n",
      "92/92 [==============================] - 0s - loss: 0.1338     \n",
      "Epoch 137/500\n",
      "92/92 [==============================] - 0s - loss: 0.1338     \n",
      "Epoch 138/500\n",
      "92/92 [==============================] - 0s - loss: 0.1339     \n",
      "Epoch 139/500\n",
      "92/92 [==============================] - 0s - loss: 0.1338     \n",
      "Epoch 140/500\n",
      "92/92 [==============================] - 0s - loss: 0.1337     \n",
      "Epoch 141/500\n",
      "92/92 [==============================] - 0s - loss: 0.1339     \n",
      "Epoch 142/500\n",
      "92/92 [==============================] - 0s - loss: 0.1338     \n",
      "Epoch 143/500\n",
      "92/92 [==============================] - 0s - loss: 0.1335     \n",
      "Epoch 144/500\n",
      "92/92 [==============================] - 0s - loss: 0.1335     \n",
      "Epoch 145/500\n",
      "92/92 [==============================] - 0s - loss: 0.1334     \n",
      "Epoch 146/500\n",
      "92/92 [==============================] - 0s - loss: 0.1334     \n",
      "Epoch 147/500\n",
      "92/92 [==============================] - 0s - loss: 0.1337     \n",
      "Epoch 148/500\n",
      "92/92 [==============================] - 0s - loss: 0.1333     \n",
      "Epoch 149/500\n",
      "92/92 [==============================] - 0s - loss: 0.1335     \n",
      "Epoch 150/500\n",
      "92/92 [==============================] - 0s - loss: 0.1334     \n",
      "Epoch 151/500\n",
      "92/92 [==============================] - 0s - loss: 0.1332     \n",
      "Epoch 152/500\n",
      "92/92 [==============================] - 0s - loss: 0.1333     \n",
      "Epoch 153/500\n",
      "92/92 [==============================] - 0s - loss: 0.1335     \n",
      "Epoch 154/500\n",
      "92/92 [==============================] - 0s - loss: 0.1331     \n",
      "Epoch 155/500\n",
      "92/92 [==============================] - 0s - loss: 0.1330     \n",
      "Epoch 156/500\n",
      "92/92 [==============================] - 0s - loss: 0.1332     \n",
      "Epoch 157/500\n",
      "92/92 [==============================] - 0s - loss: 0.1330     \n",
      "Epoch 158/500\n",
      "92/92 [==============================] - 0s - loss: 0.1332     \n",
      "Epoch 159/500\n",
      "92/92 [==============================] - 0s - loss: 0.1329     \n",
      "Epoch 160/500\n",
      "92/92 [==============================] - 0s - loss: 0.1329     \n",
      "Epoch 161/500\n",
      "92/92 [==============================] - 0s - loss: 0.1330     \n",
      "Epoch 162/500\n",
      "92/92 [==============================] - 0s - loss: 0.1329     \n",
      "Epoch 163/500\n",
      "92/92 [==============================] - 0s - loss: 0.1328     \n",
      "Epoch 164/500\n",
      "92/92 [==============================] - 0s - loss: 0.1327     \n",
      "Epoch 165/500\n",
      "92/92 [==============================] - 0s - loss: 0.1329     \n",
      "Epoch 166/500\n",
      "92/92 [==============================] - 0s - loss: 0.1327     \n",
      "Epoch 167/500\n",
      "92/92 [==============================] - 0s - loss: 0.1327     \n",
      "Epoch 168/500\n",
      "92/92 [==============================] - 0s - loss: 0.1326     \n",
      "Epoch 169/500\n",
      "92/92 [==============================] - 0s - loss: 0.1326     \n",
      "Epoch 170/500\n",
      "92/92 [==============================] - 0s - loss: 0.1327     \n",
      "Epoch 171/500\n",
      "92/92 [==============================] - 0s - loss: 0.1326     \n",
      "Epoch 172/500\n",
      "92/92 [==============================] - 0s - loss: 0.1325     \n",
      "Epoch 173/500\n",
      "92/92 [==============================] - 0s - loss: 0.1326     \n",
      "Epoch 174/500\n",
      "92/92 [==============================] - 0s - loss: 0.1324     \n",
      "Epoch 175/500\n",
      "92/92 [==============================] - 0s - loss: 0.1325     \n",
      "Epoch 176/500\n",
      "92/92 [==============================] - 0s - loss: 0.1324     \n",
      "Epoch 177/500\n",
      "92/92 [==============================] - 0s - loss: 0.1325     \n",
      "Epoch 178/500\n",
      "92/92 [==============================] - 0s - loss: 0.1324     \n",
      "Epoch 179/500\n",
      "92/92 [==============================] - 0s - loss: 0.1322     \n",
      "Epoch 180/500\n",
      "92/92 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 181/500\n",
      "92/92 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 182/500\n",
      "92/92 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 183/500\n",
      "92/92 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 184/500\n",
      "92/92 [==============================] - 0s - loss: 0.1324     \n",
      "Epoch 185/500\n",
      "92/92 [==============================] - 0s - loss: 0.1322     \n",
      "Epoch 186/500\n",
      "92/92 [==============================] - 0s - loss: 0.1321     \n",
      "Epoch 187/500\n",
      "92/92 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 188/500\n",
      "92/92 [==============================] - 0s - loss: 0.1321     \n",
      "Epoch 189/500\n",
      "92/92 [==============================] - 0s - loss: 0.1323     \n",
      "Epoch 190/500\n",
      "92/92 [==============================] - 0s - loss: 0.1320     \n",
      "Epoch 191/500\n",
      "92/92 [==============================] - 0s - loss: 0.1318     \n",
      "Epoch 192/500\n",
      "92/92 [==============================] - 0s - loss: 0.1318     \n",
      "Epoch 193/500\n",
      "92/92 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 194/500\n",
      "92/92 [==============================] - 0s - loss: 0.1322     \n",
      "Epoch 195/500\n",
      "92/92 [==============================] - 0s - loss: 0.1320     \n",
      "Epoch 196/500\n",
      "92/92 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 197/500\n",
      "92/92 [==============================] - 0s - loss: 0.1318     \n",
      "Epoch 198/500\n",
      "92/92 [==============================] - 0s - loss: 0.1320     \n",
      "Epoch 199/500\n",
      "92/92 [==============================] - 0s - loss: 0.1321     \n",
      "Epoch 200/500\n",
      "92/92 [==============================] - 0s - loss: 0.1319     \n",
      "Epoch 201/500\n",
      "92/92 [==============================] - 0s - loss: 0.1316     \n",
      "Epoch 202/500\n",
      "92/92 [==============================] - 0s - loss: 0.1318     \n",
      "Epoch 203/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 204/500\n",
      "92/92 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 205/500\n",
      "92/92 [==============================] - 0s - loss: 0.1316     \n",
      "Epoch 206/500\n",
      "92/92 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 207/500\n",
      "92/92 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 208/500\n",
      "92/92 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 209/500\n",
      "92/92 [==============================] - 0s - loss: 0.1316     \n",
      "Epoch 210/500\n",
      "92/92 [==============================] - 0s - loss: 0.1314     \n",
      "Epoch 211/500\n",
      "92/92 [==============================] - 0s - loss: 0.1314     \n",
      "Epoch 212/500\n",
      "92/92 [==============================] - 0s - loss: 0.1316     \n",
      "Epoch 213/500\n",
      "92/92 [==============================] - 0s - loss: 0.1317     \n",
      "Epoch 214/500\n",
      "92/92 [==============================] - 0s - loss: 0.1314     \n",
      "Epoch 215/500\n",
      "92/92 [==============================] - 0s - loss: 0.1313     \n",
      "Epoch 216/500\n",
      "92/92 [==============================] - 0s - loss: 0.1311     \n",
      "Epoch 217/500\n",
      "92/92 [==============================] - 0s - loss: 0.1314     \n",
      "Epoch 218/500\n",
      "92/92 [==============================] - 0s - loss: 0.1313     \n",
      "Epoch 219/500\n",
      "92/92 [==============================] - 0s - loss: 0.1312     \n",
      "Epoch 220/500\n",
      "92/92 [==============================] - 0s - loss: 0.1311     \n",
      "Epoch 221/500\n",
      "92/92 [==============================] - 0s - loss: 0.1312     \n",
      "Epoch 222/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 223/500\n",
      "92/92 [==============================] - 0s - loss: 0.1313     \n",
      "Epoch 224/500\n",
      "92/92 [==============================] - 0s - loss: 0.1312     \n",
      "Epoch 225/500\n",
      "92/92 [==============================] - 0s - loss: 0.1311     \n",
      "Epoch 226/500\n",
      "92/92 [==============================] - 0s - loss: 0.1311     \n",
      "Epoch 227/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 228/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 229/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 230/500\n",
      "92/92 [==============================] - 0s - loss: 0.1310     \n",
      "Epoch 231/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 232/500\n",
      "92/92 [==============================] - 0s - loss: 0.1310     \n",
      "Epoch 233/500\n",
      "92/92 [==============================] - 0s - loss: 0.1310     \n",
      "Epoch 234/500\n",
      "92/92 [==============================] - 0s - loss: 0.1307     \n",
      "Epoch 235/500\n",
      "92/92 [==============================] - 0s - loss: 0.1308     \n",
      "Epoch 236/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 237/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 238/500\n",
      "92/92 [==============================] - 0s - loss: 0.1307     \n",
      "Epoch 239/500\n",
      "92/92 [==============================] - 0s - loss: 0.1306     \n",
      "Epoch 240/500\n",
      "92/92 [==============================] - 0s - loss: 0.1309     \n",
      "Epoch 241/500\n",
      "92/92 [==============================] - 0s - loss: 0.1305     \n",
      "Epoch 242/500\n",
      "92/92 [==============================] - 0s - loss: 0.1305     \n",
      "Epoch 243/500\n",
      "92/92 [==============================] - 0s - loss: 0.1307     \n",
      "Epoch 244/500\n",
      "92/92 [==============================] - 0s - loss: 0.1310     \n",
      "Epoch 245/500\n",
      "92/92 [==============================] - 0s - loss: 0.1305     \n",
      "Epoch 246/500\n",
      "92/92 [==============================] - 0s - loss: 0.1305     \n",
      "Epoch 247/500\n",
      "92/92 [==============================] - 0s - loss: 0.1304     \n",
      "Epoch 248/500\n",
      "92/92 [==============================] - 0s - loss: 0.1305     \n",
      "Epoch 249/500\n",
      "92/92 [==============================] - 0s - loss: 0.1304     \n",
      "Epoch 250/500\n",
      "92/92 [==============================] - 0s - loss: 0.1303     \n",
      "Epoch 251/500\n",
      "92/92 [==============================] - 0s - loss: 0.1304     \n",
      "Epoch 252/500\n",
      "92/92 [==============================] - 0s - loss: 0.1303     \n",
      "Epoch 253/500\n",
      "92/92 [==============================] - 0s - loss: 0.1303     \n",
      "Epoch 254/500\n",
      "92/92 [==============================] - 0s - loss: 0.1303     \n",
      "Epoch 255/500\n",
      "92/92 [==============================] - 0s - loss: 0.1302     \n",
      "Epoch 256/500\n",
      "92/92 [==============================] - 0s - loss: 0.1304     \n",
      "Epoch 257/500\n",
      "92/92 [==============================] - 0s - loss: 0.1301     \n",
      "Epoch 258/500\n",
      "92/92 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 259/500\n",
      "92/92 [==============================] - 0s - loss: 0.1302     \n",
      "Epoch 260/500\n",
      "92/92 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 261/500\n",
      "92/92 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 262/500\n",
      "92/92 [==============================] - 0s - loss: 0.1301     \n",
      "Epoch 263/500\n",
      "92/92 [==============================] - 0s - loss: 0.1301     \n",
      "Epoch 264/500\n",
      "92/92 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 265/500\n",
      "92/92 [==============================] - 0s - loss: 0.1299     \n",
      "Epoch 266/500\n",
      "92/92 [==============================] - 0s - loss: 0.1298     \n",
      "Epoch 267/500\n",
      "92/92 [==============================] - 0s - loss: 0.1297     \n",
      "Epoch 268/500\n",
      "92/92 [==============================] - 0s - loss: 0.1300     \n",
      "Epoch 269/500\n",
      "92/92 [==============================] - 0s - loss: 0.1298     \n",
      "Epoch 270/500\n",
      "92/92 [==============================] - 0s - loss: 0.1298     \n",
      "Epoch 271/500\n",
      "92/92 [==============================] - 0s - loss: 0.1297     \n",
      "Epoch 272/500\n",
      "92/92 [==============================] - 0s - loss: 0.1296     \n",
      "Epoch 273/500\n",
      "92/92 [==============================] - 0s - loss: 0.1296     \n",
      "Epoch 274/500\n",
      "92/92 [==============================] - 0s - loss: 0.1297     \n",
      "Epoch 275/500\n",
      "92/92 [==============================] - 0s - loss: 0.1297     \n",
      "Epoch 276/500\n",
      "92/92 [==============================] - 0s - loss: 0.1294     \n",
      "Epoch 277/500\n",
      "92/92 [==============================] - 0s - loss: 0.1294     \n",
      "Epoch 278/500\n",
      "92/92 [==============================] - 0s - loss: 0.1297     \n",
      "Epoch 279/500\n",
      "92/92 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 280/500\n",
      "92/92 [==============================] - 0s - loss: 0.1295     \n",
      "Epoch 281/500\n",
      "92/92 [==============================] - 0s - loss: 0.1296     \n",
      "Epoch 282/500\n",
      "92/92 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 283/500\n",
      "92/92 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 284/500\n",
      "92/92 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 285/500\n",
      "92/92 [==============================] - 0s - loss: 0.1295     \n",
      "Epoch 286/500\n",
      "92/92 [==============================] - 0s - loss: 0.1295     \n",
      "Epoch 287/500\n",
      "92/92 [==============================] - 0s - loss: 0.1291     \n",
      "Epoch 288/500\n",
      "92/92 [==============================] - 0s - loss: 0.1293     \n",
      "Epoch 289/500\n",
      "92/92 [==============================] - 0s - loss: 0.1291     \n",
      "Epoch 290/500\n",
      "92/92 [==============================] - 0s - loss: 0.1292     \n",
      "Epoch 291/500\n",
      "92/92 [==============================] - 0s - loss: 0.1292     \n",
      "Epoch 292/500\n",
      "92/92 [==============================] - 0s - loss: 0.1292     \n",
      "Epoch 293/500\n",
      "92/92 [==============================] - 0s - loss: 0.1291     \n",
      "Epoch 294/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 295/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 296/500\n",
      "92/92 [==============================] - 0s - loss: 0.1291     \n",
      "Epoch 297/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 298/500\n",
      "92/92 [==============================] - 0s - loss: 0.1291     \n",
      "Epoch 299/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 300/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 301/500\n",
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 302/500\n",
      "92/92 [==============================] - 0s - loss: 0.1290     \n",
      "Epoch 303/500\n",
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 304/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 305/500\n",
      "92/92 [==============================] - 0s - loss: 0.1289     \n",
      "Epoch 306/500\n",
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 307/500\n",
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 308/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 309/500\n",
      "92/92 [==============================] - 0s - loss: 0.1286     \n",
      "Epoch 310/500\n",
      "92/92 [==============================] - 0s - loss: 0.1287     \n",
      "Epoch 311/500\n",
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 312/500\n",
      "92/92 [==============================] - 0s - loss: 0.1287     \n",
      "Epoch 313/500\n",
      "92/92 [==============================] - 0s - loss: 0.1285     \n",
      "Epoch 314/500\n",
      "92/92 [==============================] - 0s - loss: 0.1286     \n",
      "Epoch 315/500\n",
      "92/92 [==============================] - 0s - loss: 0.1286     \n",
      "Epoch 316/500\n",
      "92/92 [==============================] - 0s - loss: 0.1284     \n",
      "Epoch 317/500\n",
      "92/92 [==============================] - 0s - loss: 0.1285     \n",
      "Epoch 318/500\n",
      "92/92 [==============================] - 0s - loss: 0.1284     \n",
      "Epoch 319/500\n",
      "92/92 [==============================] - 0s - loss: 0.1283     \n",
      "Epoch 320/500\n",
      "92/92 [==============================] - 0s - loss: 0.1286     \n",
      "Epoch 321/500\n",
      "92/92 [==============================] - 0s - loss: 0.1288     \n",
      "Epoch 322/500\n",
      "92/92 [==============================] - 0s - loss: 0.1283     \n",
      "Epoch 323/500\n",
      "92/92 [==============================] - 0s - loss: 0.1285     \n",
      "Epoch 324/500\n",
      "92/92 [==============================] - 0s - loss: 0.1283     \n",
      "Epoch 325/500\n",
      "92/92 [==============================] - 0s - loss: 0.1284     \n",
      "Epoch 326/500\n",
      "92/92 [==============================] - 0s - loss: 0.1284     \n",
      "Epoch 327/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 328/500\n",
      "92/92 [==============================] - 0s - loss: 0.1283     \n",
      "Epoch 329/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 330/500\n",
      "92/92 [==============================] - 0s - loss: 0.1282     \n",
      "Epoch 331/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 332/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 333/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 334/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 335/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 336/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 337/500\n",
      "92/92 [==============================] - 0s - loss: 0.1282     \n",
      "Epoch 338/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 339/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 340/500\n",
      "92/92 [==============================] - 0s - loss: 0.1282     \n",
      "Epoch 341/500\n",
      "92/92 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 342/500\n",
      "92/92 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 343/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 344/500\n",
      "92/92 [==============================] - 0s - loss: 0.1280     \n",
      "Epoch 345/500\n",
      "92/92 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 346/500\n",
      "92/92 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 347/500\n",
      "92/92 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 348/500\n",
      "92/92 [==============================] - 0s - loss: 0.1278     \n",
      "Epoch 349/500\n",
      "92/92 [==============================] - 0s - loss: 0.1281     \n",
      "Epoch 350/500\n",
      "92/92 [==============================] - 0s - loss: 0.1277     \n",
      "Epoch 351/500\n",
      "92/92 [==============================] - 0s - loss: 0.1277     \n",
      "Epoch 352/500\n",
      "92/92 [==============================] - 0s - loss: 0.1277     \n",
      "Epoch 353/500\n",
      "92/92 [==============================] - 0s - loss: 0.1277     \n",
      "Epoch 354/500\n",
      "92/92 [==============================] - 0s - loss: 0.1277     \n",
      "Epoch 355/500\n",
      "92/92 [==============================] - 0s - loss: 0.1276     \n",
      "Epoch 356/500\n",
      "92/92 [==============================] - 0s - loss: 0.1277     \n",
      "Epoch 357/500\n",
      "92/92 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 358/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 359/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 360/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 361/500\n",
      "92/92 [==============================] - 0s - loss: 0.1276     \n",
      "Epoch 362/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 363/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 364/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 365/500\n",
      "92/92 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 366/500\n",
      "92/92 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 367/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 368/500\n",
      "92/92 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 369/500\n",
      "92/92 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 370/500\n",
      "92/92 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 371/500\n",
      "92/92 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 372/500\n",
      "92/92 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 373/500\n",
      "92/92 [==============================] - 0s - loss: 0.1275     \n",
      "Epoch 374/500\n",
      "92/92 [==============================] - 0s - loss: 0.1271     \n",
      "Epoch 375/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 376/500\n",
      "92/92 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 377/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 378/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 379/500\n",
      "92/92 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 380/500\n",
      "92/92 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 381/500\n",
      "92/92 [==============================] - 0s - loss: 0.1274     \n",
      "Epoch 382/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 383/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 384/500\n",
      "92/92 [==============================] - 0s - loss: 0.1271     \n",
      "Epoch 385/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 386/500\n",
      "92/92 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 387/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 388/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 389/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 390/500\n",
      "92/92 [==============================] - 0s - loss: 0.1272     \n",
      "Epoch 391/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 392/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 393/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 394/500\n",
      "92/92 [==============================] - 0s - loss: 0.1268     \n",
      "Epoch 395/500\n",
      "92/92 [==============================] - 0s - loss: 0.1271     \n",
      "Epoch 396/500\n",
      "92/92 [==============================] - 0s - loss: 0.1268     \n",
      "Epoch 397/500\n",
      "92/92 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 398/500\n",
      "92/92 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 399/500\n",
      "92/92 [==============================] - 0s - loss: 0.1268     \n",
      "Epoch 400/500\n",
      "92/92 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 401/500\n",
      "92/92 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 402/500\n",
      "92/92 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 403/500\n",
      "92/92 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 404/500\n",
      "92/92 [==============================] - 0s - loss: 0.1270     \n",
      "Epoch 405/500\n",
      "92/92 [==============================] - 0s - loss: 0.1266     \n",
      "Epoch 406/500\n",
      "92/92 [==============================] - 0s - loss: 0.1268     \n",
      "Epoch 407/500\n",
      "92/92 [==============================] - 0s - loss: 0.1266     \n",
      "Epoch 408/500\n",
      "92/92 [==============================] - 0s - loss: 0.1266     \n",
      "Epoch 409/500\n",
      "92/92 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 410/500\n",
      "92/92 [==============================] - 0s - loss: 0.1268     \n",
      "Epoch 411/500\n",
      "92/92 [==============================] - 0s - loss: 0.1265     \n",
      "Epoch 412/500\n",
      "92/92 [==============================] - 0s - loss: 0.1269     \n",
      "Epoch 413/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 414/500\n",
      "92/92 [==============================] - 0s - loss: 0.1265     \n",
      "Epoch 415/500\n",
      "92/92 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 416/500\n",
      "92/92 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 417/500\n",
      "92/92 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 418/500\n",
      "92/92 [==============================] - 0s - loss: 0.1265     \n",
      "Epoch 419/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 420/500\n",
      "92/92 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 421/500\n",
      "92/92 [==============================] - 0s - loss: 0.1265     \n",
      "Epoch 422/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 423/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 424/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 425/500\n",
      "92/92 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 426/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 427/500\n",
      "92/92 [==============================] - 0s - loss: 0.1266     \n",
      "Epoch 428/500\n",
      "92/92 [==============================] - 0s - loss: 0.1264     \n",
      "Epoch 429/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 430/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 431/500\n",
      "92/92 [==============================] - 0s - loss: 0.1265     \n",
      "Epoch 432/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 433/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 434/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 435/500\n",
      "92/92 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 436/500\n",
      "92/92 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 437/500\n",
      "92/92 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 438/500\n",
      "92/92 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 439/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 440/500\n",
      "92/92 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 441/500\n",
      "92/92 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 442/500\n",
      "92/92 [==============================] - 0s - loss: 0.1263     \n",
      "Epoch 443/500\n",
      "92/92 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 444/500\n",
      "92/92 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 445/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 446/500\n",
      "92/92 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 447/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 448/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 449/500\n",
      "92/92 [==============================] - 0s - loss: 0.1262     \n",
      "Epoch 450/500\n",
      "92/92 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 451/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 452/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 453/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 454/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 455/500\n",
      "92/92 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 456/500\n",
      "92/92 [==============================] - 0s - loss: 0.1260     \n",
      "Epoch 457/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 458/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 459/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 460/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 461/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 462/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 463/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 464/500\n",
      "92/92 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 465/500\n",
      "92/92 [==============================] - 0s - loss: 0.1259     \n",
      "Epoch 466/500\n",
      "92/92 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 467/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 468/500\n",
      "92/92 [==============================] - 0s - loss: 0.1258     \n",
      "Epoch 469/500\n",
      "92/92 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 470/500\n",
      "92/92 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 471/500\n",
      "92/92 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 472/500\n",
      "92/92 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 473/500\n",
      "92/92 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 474/500\n",
      "92/92 [==============================] - 0s - loss: 0.1255     \n",
      "Epoch 475/500\n",
      "92/92 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 476/500\n",
      "92/92 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 477/500\n",
      "92/92 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 478/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 479/500\n",
      "92/92 [==============================] - 0s - loss: 0.1255     \n",
      "Epoch 480/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 481/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 482/500\n",
      "92/92 [==============================] - 0s - loss: 0.1256     \n",
      "Epoch 483/500\n",
      "92/92 [==============================] - 0s - loss: 0.1257     \n",
      "Epoch 484/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 485/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 486/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 487/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 488/500\n",
      "92/92 [==============================] - 0s - loss: 0.1255     \n",
      "Epoch 489/500\n",
      "92/92 [==============================] - 0s - loss: 0.1251     \n",
      "Epoch 490/500\n",
      "92/92 [==============================] - 0s - loss: 0.1254     \n",
      "Epoch 491/500\n",
      "92/92 [==============================] - 0s - loss: 0.1253     \n",
      "Epoch 492/500\n",
      "92/92 [==============================] - 0s - loss: 0.1251     \n",
      "Epoch 493/500\n",
      "92/92 [==============================] - 0s - loss: 0.1253     \n",
      "Epoch 494/500\n",
      "92/92 [==============================] - 0s - loss: 0.1253     \n",
      "Epoch 495/500\n",
      "92/92 [==============================] - 0s - loss: 0.1253     \n",
      "Epoch 496/500\n",
      "92/92 [==============================] - 0s - loss: 0.1253     \n",
      "Epoch 497/500\n",
      "92/92 [==============================] - 0s - loss: 0.1252     \n",
      "Epoch 498/500\n",
      "92/92 [==============================] - 0s - loss: 0.1251     \n",
      "Epoch 499/500\n",
      "92/92 [==============================] - 0s - loss: 0.1252     \n",
      "Epoch 500/500\n",
      "92/92 [==============================] - 0s - loss: 0.1253     \n"
     ]
    }
   ],
   "source": [
    "train_diff = difference(train, 1)\n",
    "test_diff = difference(test, 1)\n",
    "train_matrix = series_to_supervised(train_diff, lstm_num_timesteps, lstm_num_predictions).values\n",
    "test_matrix = series_to_supervised(test_diff, lstm_num_timesteps, lstm_num_predictions).values\n",
    "scaler, train_scaled, test_scaled = scale(train_matrix, test_matrix)\n",
    "\n",
    "if lstm_stateful:\n",
    "    lstm_model = fit_lstm_stateful(train_scaled, batchsize, num_epochs, num_neurons)\n",
    "else:\n",
    "    lstm_model = fit_lstm_stateless(train_scaled, batchsize, num_epochs, num_neurons)\n",
    "        \n",
    "train_predictions = list()\n",
    "for i in range(len(train_scaled)):\n",
    "    X, y = train_scaled[i, 0:lstm_num_timesteps], train_scaled[i, lstm_num_timesteps:]\n",
    "    yhat = forecast_lstm(lstm_model, 1, X)\n",
    "    yhat = invert_scale(scaler, X, yhat)\n",
    "    yhat = inverse_difference(train, yhat, len(train_scaled)+1-i)\n",
    "    train_predictions.append(yhat)\n",
    "            \n",
    "test_predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "    X, y = train_scaled[i, 0:lstm_num_timesteps], train_scaled[i, lstm_num_timesteps:]\n",
    "    yhat = forecast_lstm(lstm_model, 1, X)\n",
    "    yhat = invert_scale(scaler, X, yhat)\n",
    "    yhat = inverse_difference(test, yhat, len(test_scaled)+1-i)\n",
    "    test_predictions.append(yhat)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 114.55276816,  115.67039847,  113.51375201,  114.82428757]),\n",
       " array([ 117.96469106,  117.41003363,  119.55561744,  118.67197347]),\n",
       " array([ 123.29748414,  123.91913655,  123.32268702,  124.1357005 ]),\n",
       " array([ 124.29695169,  122.99178074,  124.64701184,  122.06088905]),\n",
       " array([ 125.12049777,  122.60947415,  124.40177033,  127.02602864]),\n",
       " array([ 124.09604297,  123.77633317,  124.57936791,  124.54257149]),\n",
       " array([ 125.48332493,  126.34720981,  124.96748768,  125.97355549]),\n",
       " array([ 124.63579619,  124.79778025,  125.7266907 ,  124.23497449]),\n",
       " array([ 127.50200298,  128.56589068,  126.49602559,  127.91528113]),\n",
       " array([ 129.33847646,  128.77850994,  130.40324253,  131.72731751])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 19.66373202,  20.78136233,  18.62471587,  19.93525143]),\n",
       " array([ 19.7901582 ,  19.23550077,  21.38108458,  20.4974406 ]),\n",
       " array([ 21.26706776,  21.88872017,  21.29227064,  22.10528411]),\n",
       " array([ 21.41894242,  20.11377147,  21.76900257,  19.18287978]),\n",
       " array([ 21.96777524,  19.45675162,  21.2490478 ,  23.87330611]),\n",
       " array([ 25.26527782,  24.94556803,  25.74860276,  25.71180635]),\n",
       " array([ 23.88206567,  24.74595056,  23.36622842,  24.37229623]),\n",
       " array([ 23.14237311,  23.30435718,  24.23326762,  22.74155141]),\n",
       " array([ 23.03341283,  24.09730053,  22.02743543,  23.44669098]),\n",
       " array([ 27.45994206,  26.89997554,  28.52470813,  29.8487831 ])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(predicted_data, true_data):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    padding = [np.nan for i in range(effective_timesteps)]\n",
    "    plt.plot(padding + predicted_data, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results_multiple(predicted_data, true_data, prediction_window):\n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(true_data, label='True Data')\n",
    "    for i, data in enumerate(predicted_data):\n",
    "        padding = np.array([np.nan for p in range((i+1) * 4)])\n",
    "        plt.plot(np.append(padding,data))\n",
    "    #plt.savefig(fig_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk0QvhBCDnCR6IYQY5CTRCyHEIPf/WPHiLMcKtfQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe914402080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results_multiple(train_predictions, train, lstm_num_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe90c6c8f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results_multiple(test_predictions, test, lstm_num_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
