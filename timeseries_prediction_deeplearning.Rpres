```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
opts_chunk$set(fig.align='left')
```

<style>

.reveal div {
  font-size: 1em;
}

.exclaim .reveal .state-background {
  background: black;
} 

.exclaim .reveal h1,
.exclaim .reveal h2,
.exclaim .reveal p {
  color: white;
}

.reveal pre code {
  font-size: 0.5em;
}

.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:90%;
    margin-top:-150px;
}
.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
</style>


Time series forecasting - with deep learning
========================================================
author: Sigrid Keydana
date: 2017-05-23
autosize: true 



Wait.
========================================================

&nbsp;

- What's a time series?
- What's deep learning?
- How I can use deep learning for time series prediction?


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Time series forecasting: the classical approach</h1>


A time series 
========================================================

&nbsp;

```{r, echo=FALSE, warning=FALSE, message=FALSE}
reticulate::use_condaenv("tf3.5", required = TRUE)
library(kerasR)
library(forecast)
library(dplyr)
library(ggplot2)
library(readr)
library(gridExtra)
library(ggfortify)
library(tidyr)
library(lubridate)
```


```{r, fig.width=20,fig.height=10}
cola_df <- read_csv("monthly-sales-of-tasty-cola.csv", col_names = c("month", "sales"), skip = 1, 
                    col_types = cols(month = col_date("%y-%m")))
ggplot(cola_df, aes(x = month, y = sales)) + geom_line() + ggtitle("Monthly sales of Tasty Cola")
```


Another one ...
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
traffic_df <- read_csv("internet-traffic-data-in-bits-fr.csv", col_names = c("hour", "bits"), skip = 1)
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Internet traffic")
```


And another.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
win_df <- read_csv("winning-times-for-the-mens-400-m.csv", col_names = c("year", "seconds"), skip = 1)
ggplot(win_df, aes(x = year, y = seconds)) + geom_line() + ggtitle("Men's 400m winning times")
```


Sometimes we may look at several time series together.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
deaths_df <- read_csv("deaths-from-homicides-and-suicid.csv", col_names = c("year", "homicide", "suicide"), skip = 1)
deaths_df <- gather(deaths_df, key = 'type', value = 'deaths', homicide:suicide)
ggplot(deaths_df, aes(x = year, y = deaths, color = type)) + geom_line() + scale_colour_manual(values=c("green","blue")) + ggtitle("Australia: Homicides and suicides")
```


Sometimes there's more to the picture than we might think.
========================================================

&nbsp;

_So far, this is nothing but a univariate time series of lynx population._

```{r, fig.width=20,fig.height=8}
data("lynx")
autoplot(lynx) + ggtitle("Lynx population over time")
```


However ...
========================================================

&nbsp;

<table>
<tr>
<td><img src='Lynx.gif' width='600px' /></td>
</tr>
<tr><td style='font-size: 0.6em;'>Source: Rudolfo's Usenet Animal Pictures Gallery (link no longer exists)</td></tr>
</table>


Lynx, in context
========================================================


```{r, fig.width=20,fig.height=10}
lynx_df <- read_delim("lynxhare.csv", delim = ";") %>% select(year, hare, lynx) %>%   filter(between(year, 1890, 1945)) %>% mutate(hare = scale(hare), lynx = scale(lynx))
lynx_df <- gather(lynx_df, key = 'species', value = 'number', hare:lynx)
ggplot(lynx_df, aes(x = year, y = number, color = species)) + geom_line() + scale_colour_manual(values=c("green","red")) + ggtitle("Lynx and hare populations over time")
```



Concepts in classical time series modeling
========================================================

&nbsp;

- Stationarity 
- Decomposition
- Autocorrelation



Wait. This will be about deep learning
========================================================

&nbsp;

... why would the classical approach even matter?


Stationarity (1)
========================================================

&nbsp;

- We want to forecast future values of a time series
- We need fundamental statistical properties like mean, variance ...
- What _is_ the mean, or the variance, of a time series?




Stationarity (2)
========================================================

&nbsp;

- If a time series $y_{t}$ is stationary, then for all $s$, the distribution of ($y_{t}$,â€¦, $y_{t+s}$) does not depend on $t$.
- By ergodicity, after we remove any trend and seasonality effects, we may assume that the residual series is stationary in the mean: $\mu(t)=t$


Time series decomposition
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
autoplot(stl(ts(cola_df$sales, frequency=12), s.window = 12))
```


TBD: differencing / lograithm!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
========================================================

&nbsp;



Autocorrelation - Case 1: White noise
========================================================

&nbsp;

If consecutive values were not related, there'd be no way of forecasting

```{r, fig.width=20,fig.height=10}
s1 <- ts(rnorm(100))
ts1 <- autoplot(s1)
acf1 <- ggfortify:::autoplot.acf(acf(s1, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
do.call('grid.arrange', list('grobs' = list(ts1, acf1), 'ncol' = 2, top = "White noise"))

```


Autocorrelation - Case 2: Linear trend
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
s2 <- ts(1:100 + rnorm(100, 2, 4))
ts2 <- autoplot(s2)
acf2 <- ggfortify:::autoplot.acf(acf(s2, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
do.call('grid.arrange', list('grobs' = list(ts2, acf2), 'ncol' = 2, top = "Series with a trend"))
```


Autocorrelation - Case 3: Seasonality
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
s3 <- ts(rep(1:5,20) + rnorm(100, sd= 0.5))
ts3 <- autoplot(s3)
acf3 <- ggfortify:::autoplot.acf(acf(s3, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
do.call('grid.arrange', list('grobs' = list(ts3, acf3), 'ncol' = 2, top = "Series with seasonality"))
```



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Internet traffic forecasted, the classical way  </h1>



Here's the traffic time series again.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Internet traffic")
```


Let's first look at decomposition.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
traffic_ts <- msts(traffic_df$bits,seasonal.periods = c(24, 24*7))
autoplot(stl(traffic_ts, s.window = 7 * 24))
```


How about autocorrelation?
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
ggfortify:::autoplot.acf(acf(traffic_ts, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
```



The usual ARIMA won't work...
========================================================

&nbsp;

```{r, error = TRUE}
arima_fit <- auto.arima(traffic_ts, stepwise = FALSE, max.order = 10, trace = TRUE)
```


... will regression with ARIMA errors?
========================================================

&nbsp;

Let's add an indicator variable for whether it's weekend. 

```{r,  fig.width=20,fig.height=10}
traffic_df_wd <- traffic_df %>% mutate(weekend = if_else(wday(hour) %in% c(7,1), 1, 0))
ggplot(traffic_df_wd, aes(x=hour, y=bits, color=factor(weekend))) + geom_point()
#arima_fit <- auto.arima(ts(traffic_df_wd$bits, frequency = 24 * 7), xreg = traffic_df_wd$weekend, 
                        #stepwise = FALSE, max.order = 10, trace = TRUE)
```

No.
========================================================

&nbsp;

This will run forever and you'll have to kill it.

```{r,  fig.width=20,fig.height=10}
# arima_fit <- auto.arima(ts(traffic_df_wd$bits, frequency = 24 * 7), xreg = traffic_df_wd$weekend, 
#                            stepwise = FALSE, max.order = 10, trace = TRUE)
```


Trying TBATS
========================================================


TBATS ("Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components") does _not_ fail... 

But, look at the forecast.

```{r, fig.width=20,fig.height=8}
tbats_fit <- tbats(traffic_ts)
plot(forecast(tbats_fit, h=14*24))
```



Will deep learning do any better for this time series?
========================================================

&nbsp;

- Let's step back a little though. 
- How does deep learning even do time series?
- What even is deep learning?

========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Deep Learning in a nutshell</h1>


========================================================

&nbsp;

```{r}


```


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>LSTM for time series: Brave new world?</h1>


New world, new rules?
========================================================

&nbsp;

- Do we still care about stationarity and decomposition?
- How does DL handle trends, or seasonality?


Let's compare ARIMA vs. LSTM on a little set of benchmarks
========================================================

&nbsp;

- synthetic dataset, with trend only, test data out-of-range
- synthetic dataset, with trend only, test data in-range
- synthetic dataset, seasonal only


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>ARIMA vs. LSTM, Round 1: Trend-only dataset, test data out-of-range</h1>


Trend-only dataset 
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()
```


Trend-only dataset: Enter: ARIMA (1)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
fit <- auto.arima(trend_train)
fit
predictions <- forecast(fit, h = 20)
accuracy(predictions, trend_test)
```


Trend-only dataset: Enter: ARIMA (2)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
autoplot(predictions)
```


Trend-only dataset: Enter: ARIMA (3)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 fitted = c(fit$fitted, rep(NA, length(trend_test))),
                 preds = c(rep(NA, length(trend_train)), predictions$mean),
                 lower = c(rep(NA, length(trend_train)), unclass(predictions$lower)[,2]),
                 upper = c(rep(NA, length(trend_train)), unclass(predictions$upper)[,2]))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + 
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1)
```


Trend-only dataset: Enter: LSTM 
========================================================

&nbsp;

- Let's first show what we have to do for time series prediction with LSTM networks.

- We'll choose the <a href='https://keras.io/'>Keras</a> framework, and the R bindings provided by <a href='https://github.com/statsmaths/kerasR'>kerasR</a>.



Background: Data preparation for LSTM in Keras (1)
========================================================

&nbsp;

Firstly, LSTMs work with a sliding window of input, so we need to provide the data (train and test) in "window form":

```{r}
# example given for training set, - do the same for test set
# length(trend_train)
lstm_num_timesteps <- 5
X_train <- t(sapply(1:(length(trend_train) - lstm_num_timesteps), function(x) trend_train[x:(x + lstm_num_timesteps - 1)]))
# dim(X_train)
X_train[1:5, ]

y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train)), function(x) trend_train[x])
y_train[1:5]
```

```{r, echo=FALSE}
X_test <- t(sapply(1:(length(trend_test) - lstm_num_timesteps), function(x) trend_test[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test)), function(x) trend_test[x])
```


Background: Data preparation for LSTM in Keras (2)
========================================================

&nbsp;

Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)

```{r}
# example given for training set, - do the same for test set
# add 3rd dimension
dim(X_train)
X_train <- expand_dims(X_train, axis = 2)
dim(X_train)

# LSTM input shape: (samples, time steps, features)
num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]
c(num_samples, num_steps, num_features)
```

```{r, echo=FALSE}
X_test <- expand_dims(X_test, axis = 2)
```


Background: Keras - build the model
========================================================

&nbsp;


```{r}
model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
```


Background: Keras - fit the model!
========================================================

&nbsp;

```{r}
# keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```

We'll load the fitted model instead, and get the predictions.
========================================================

&nbsp;

```{r}
# we'll load the fitted model instead...
# keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('trend_nodiff.h5')
pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
```



Hm. Whatever happened to predicting the test data?
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


Anything we could have done to our data to make this work better?
========================================================

&nbsp;

- What if we had worked with the value differences, instead of the original values (learning from ARIMA & co.)?
- Or even the relative differences?
- What if we had scaled the data?


What if we work with differenced data?
========================================================

&nbsp;

```{r}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)
trend_test_diff

lstm_num_timesteps <- 4
```

```{r, echo=FALSE}
X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```


Get the predictions
========================================================


&nbsp;

```{r}
# we'll load the fitted model instead...
model <- keras_load('trend_diff.h5')
pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

# "undiff"
pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]
```


Diff makes the difference...
========================================================


&nbsp;

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2))
test_rsme 
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


Just for completeness, let's try relative differences as well
========================================================

&nbsp;


```{r}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)/trend_train[-length(trend_train)]
trend_test_diff <- diff(trend_test)/trend_test[-length(trend_test)]
trend_test_diff

lstm_num_timesteps <- 4
```

```{r, echo=FALSE}
X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```

Get the predictions
========================================================

```{r}
# we'll load the fitted model instead...
model <- keras_load('trend_reldiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
pred_train_undiff <- pred_train * trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)] + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test * trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)] + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]
```


TBD
========================================================

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)

test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2))
test_rsme

ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


What if we diff AND normalize?
========================================================

&nbsp;


```{r}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

minval <- min(trend_train_diff)
maxval <- max(trend_train_diff)

normalize <- function(vec, min, max) {
  (vec-min) / (max-min)
}
denormalize <- function(vec,min,max) {
  vec * (max - min) + min
}

trend_train_diff <- normalize(trend_train_diff, minval, maxval)
trend_test_diff <- normalize(trend_test_diff, minval, maxval)

lstm_num_timesteps <- 4
```


```{r, echo=FALSE}
X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```


Get the predictions
========================================================

```{r}
# we'll load the fitted model instead...
model <- keras_load('trend_diffnorm.h5')
pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

```


TBD
========================================================

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2))
test_rsme # 6.84

ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>ARIMA vs. LSTM, Round 2: Trend-only dataset, test data in-range</h1>




Would anything change if the test data were in the range already known by the model?
========================================================

&nbsp;


```{r,fig.width=20,fig.height=10}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)
df <- data_frame(time_id = 1:120,
train = c(trend_train, rep(NA, length(trend_test))),
test = c(rep(NA, length(trend_train)), trend_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()

```


Trend-only dataset, test data in-range: Enter: ARIMA
========================================================

&nbsp;

```{r}
fit <- auto.arima(trend_train)
new_starting_values <- trend_test[1:5] 
new_fit <- Arima(new_starting_values, model = fit)
predictions <- simulate.Arima(new_fit, nsim = 20, future = T) 
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 fitted = c(fit$fitted, rep(NA, length(trend_test))),
                 preds = c(rep(NA, length(trend_train)), predictions))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) 
```


Trend-only dataset, test data in-range: Enter: LSTM (no diffing)
========================================================

&nbsp;


```{r, echo=FALSE}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)

lstm_num_timesteps <- 5

X_train <- t(sapply(1:(length(trend_train) - lstm_num_timesteps), function(x) trend_train[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train)), function(x) trend_train[x])
X_test <- t(sapply(1:(length(trend_test) - lstm_num_timesteps), function(x) trend_test[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test)), function(x) trend_test[x])
# add 3rd dimension
X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)
# LSTM input shape: (samples, time steps, features)
num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')

#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('trend_i_nodiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
```

```{r,fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps) - pred_test)^2))
test_rsme 
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



Does it get even better with diffing?
========================================================

&nbsp;


```{r, echo=FALSE}

```




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>ARIMA vs. LSTM, Round 2: seasonal-only dataset (1)</h1>


Seasonal-only dataset 
========================================================

&nbsp;

```{r}
seasonal_train <- rep(1:7, times = 13) + rnorm(91)
seasonal_test <- rep(1:7, times = 3) + rnorm(21)
df <- data_frame(time_id = 1:113,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()
```

