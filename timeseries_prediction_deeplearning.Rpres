```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
opts_chunk$set(fig.align='left')
```

<style>

.reveal div {
  font-size: 1em;
}

.exclaim .reveal .state-background {
  background: black;
} 

.exclaim .reveal h1,
.exclaim .reveal h2,
.exclaim .reveal p {
  color: white;
}

.reveal pre code {
  font-size: 0.8em;
}

.footer {
    position: fixed; 
    top: 90%;
    text-align:right; 
    width:90%;
    margin-top:-150px;
}
.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
</style>


Time series forecasting - with deep learning
========================================================
author: Sigrid Keydana, Trivadis GmbH
date: 2017-05-23
autosize: true 
width: 1000


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Time series forecasting: the classical approach</h1>

A time series 
========================================================

&nbsp;

```{r, echo=FALSE, warning=FALSE, message=FALSE}
reticulate::use_condaenv("tf3.5", required = TRUE)
library(kerasR)
library(forecast)
library(dplyr)
library(ggplot2)
library(readr)
library(gridExtra)
library(ggfortify)
library(tidyr)
library(lubridate)
```


```{r, fig.width=20,fig.height=10}
cola_df <- read_csv("monthly-sales-of-tasty-cola.csv", col_names = c("month", "sales"), skip = 1, 
                    col_types = cols(month = col_date("%y-%m")))
ggplot(cola_df, aes(x = month, y = sales)) + geom_line() + ggtitle("Monthly sales of Tasty Cola")
```


Another one ...
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
traffic_df <- read_csv("internet-traffic-data-in-bits-fr.csv", col_names = c("hour", "bits"), skip = 1)
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Internet traffic")
```


And another.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
win_df <- read_csv("winning-times-for-the-mens-400-m.csv", col_names = c("year", "seconds"), skip = 1)
ggplot(win_df, aes(x = year, y = seconds)) + geom_line() + ggtitle("Men's 400m winning times")
```


Sometimes we may look at several time series together.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
deaths_df <- read_csv("deaths-from-homicides-and-suicid.csv", col_names = c("year", "homicide", "suicide"), skip = 1)
deaths_df <- gather(deaths_df, key = 'type', value = 'deaths', homicide:suicide)
ggplot(deaths_df, aes(x = year, y = deaths, color = type)) + geom_line() + scale_colour_manual(values=c("green","blue")) + ggtitle("Australia: Homicides and suicides")
```


Sometimes there's more to the picture than we might think.
========================================================

&nbsp;

_So far, this is nothing but a univariate time series of lynx population._

```{r, fig.width=20,fig.height=8}
data("lynx")
autoplot(lynx) + ggtitle("Lynx population over time")
```


However ...
========================================================

&nbsp;

<table>
<tr>
<td><img src='Lynx.gif' width='600px' /></td>
</tr>
<tr><td style='font-size: 0.6em;'>Source: Rudolfo's Usenet Animal Pictures Gallery (link no longer exists)</td></tr>
</table>


Lynx and hare
========================================================


```{r, fig.width=20,fig.height=10}
lynx_df <- read_delim("lynxhare.csv", delim = ";") %>% select(year, hare, lynx) %>%   filter(between(year, 1890, 1945)) %>% mutate(hare = scale(hare), lynx = scale(lynx))
lynx_df <- gather(lynx_df, key = 'species', value = 'number', hare:lynx)
ggplot(lynx_df, aes(x = year, y = number, color = species)) + geom_line() + scale_colour_manual(values=c("green","red")) + ggtitle("Lynx and hare populations over time")
```



Concepts in classical time series modeling
========================================================

&nbsp;

- Stationarity 
- Decomposition
- Autocorrelation



Wait. This will be about deep learning
========================================================

&nbsp;

... why would the classical approach even matter?


Stationarity (1)
========================================================

&nbsp;

- We want to forecast future values of a time series
- We need fundamental statistical properties like mean, variance ...
- What _is_ the mean, or the variance, of a time series?




Stationarity (2)
========================================================

&nbsp;

- If a time series $y_{t}$ is stationary, then for all $s$, the distribution of ($y_{t}$,â€¦, $y_{t+s}$) does not depend on $t$.
- By ergodicity, after we remove any trend and seasonality effects, we may assume that the residual series is stationary in the mean: $\mu(t)=t$


Differencing
========================================================

- The trend is usually removed using differencing (forming the differences of neighboring values)

```{r, fig.width=20,fig.height=10}
set.seed(7777)
trend <- 1:100 + rnorm(100, sd = 5)
diff <- diff(trend)
df <- data_frame(time_id = 1:100,
                 trend = trend,
                 diff = c(NA, diff))
df <- df %>% gather(key = 'trend_diff', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = trend_diff)) + geom_line()
```


Time series decomposition
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
autoplot(stl(ts(cola_df$sales, frequency=12), s.window = 12))
```



Autocorrelation - Case 1: White noise
========================================================

If consecutive values were not related, there'd be no way of forecasting future values

```{r, fig.width=20,fig.height=10}
s1 <- ts(rnorm(100))
ts1 <- autoplot(s1)
acf1 <- ggfortify:::autoplot.acf(acf(s1, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
do.call('grid.arrange', list('grobs' = list(ts1, acf1), 'ncol' = 2, top = "White noise"))

```


Autocorrelation - Case 2: Linear trend
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
s2 <- ts(1:100 + rnorm(100, 2, 4))
ts2 <- autoplot(s2)
acf2 <- ggfortify:::autoplot.acf(acf(s2, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
do.call('grid.arrange', list('grobs' = list(ts2, acf2), 'ncol' = 2, top = "Series with a trend"))
```


Autocorrelation - Case 3: Seasonality
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
s3 <- ts(rep(1:5,20) + rnorm(100, sd= 0.5))
ts3 <- autoplot(s3)
acf3 <- ggfortify:::autoplot.acf(acf(s3, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
do.call('grid.arrange', list('grobs' = list(ts3, acf3), 'ncol' = 2, top = "Series with seasonality"))
```



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Forecasting internet traffic, the classical way</h1>



Here's the traffic time series again.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Internet traffic")
```


Let's first look at decomposition.
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
traffic_ts <- msts(traffic_df$bits,seasonal.periods = c(24, 24*7))
autoplot(stl(traffic_ts, s.window = 7 * 24))
```


How about autocorrelation?
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
ggfortify:::autoplot.acf(acf(traffic_ts, plot = FALSE), conf.int.fill = '#00cccc', conf.int.value = 0.95)
```



The usual ARIMA won't work...
========================================================

&nbsp;

```{r, error = TRUE}
arima_fit <- auto.arima(traffic_ts, stepwise = FALSE, max.order = 10, trace = TRUE)
```


... will regression with ARIMA errors?
========================================================

&nbsp;

Let's add an indicator variable for whether it's weekend. 

```{r,  fig.width=20,fig.height=10}
traffic_df_wd <- traffic_df %>% mutate(weekend = if_else(wday(hour) %in% c(7,1), 1, 0))
ggplot(traffic_df_wd, aes(x=hour, y=bits, color=factor(weekend))) + geom_point()
```

No.
========================================================

&nbsp;

This will run forever and you'll have to kill it.

```{r,  fig.width=20,fig.height=10}
# arima_fit <- auto.arima(ts(traffic_df_wd$bits, frequency = 24 * 7), xreg = traffic_df_wd$weekend, 
#                            stepwise = FALSE, max.order = 10, trace = TRUE)
```


Trying TBATS
========================================================


TBATS ("Exponential smoothing state space model with Box-Cox transformation, ARMA errors, Trend and Seasonal components") does _not_ fail... 

But, look at the forecast.

```{r, fig.width=20,fig.height=8}
tbats_fit <- tbats(traffic_ts)
plot(forecast(tbats_fit, h=14*24))
```



Will deep learning do any better for this time series?
========================================================

&nbsp;

- Let's step back a little though. 
- How does deep learning even do time series?


Enter: LSTM (Long Short Term Memory)
========================================================

&nbsp;

<table>
<tr>
<td><img src='lstm_olah.png' width='600px' /></td><td><img src='lstm2.png' width='600px'/></td>
</tr>
<tr><td class='src'>Source: <a href='http://colah.github.io/posts/2015-08-Understanding-LSTMs/'>Christopher Olah's post on LSTM</a></td></tr>
</table>

New world, new rules?
========================================================

&nbsp;

- Do we still care about stationarity and decomposition?
- How does DL handle trends, or seasonality?


Let's compare ARIMA vs. LSTM on a little set of benchmarks
========================================================

&nbsp;

- synthetic dataset, with trend only, test data out-of-range
- synthetic dataset, with trend only, test data in-range
- synthetic dataset, seasonal only


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>ARIMA vs. LSTM, Round 1: Trend-only dataset, test data out-of-range</h1>


Trend-only dataset 
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)
df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()
```


Trend-only dataset: Enter: ARIMA (1)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)
h <- 1
n <- length(trend_test) - h + 1
fit <- auto.arima(trend_train)

# re-estimate the model as new data arrives, as per https://robjhyndman.com/hyndsight/rolling-forecasts/ 
order <- arimaorder(fit)
predictions <- matrix(0, nrow=n, ncol=h)
lower <- matrix(0, nrow=n, ncol=h) # 95% prediction interval
upper <- matrix(0, nrow=n, ncol=h)
for(i in 1:n) {  
  x <- c(trend_train[(1+i):length(trend_train)], trend_test[1:i])
  refit <- Arima(x, order=order[1:3], seasonal=order[4:6])
  predictions[i,] <- forecast(refit, h=h)$mean
  lower[i,] <- unclass(forecast(refit, h=h)$lower)[,2]
  upper[i,] <- unclass(forecast(refit, h=h)$upper)[,2]
}

(test_rsme <- sqrt(sum((trend_test - predictions)^2))) 
```


Trend-only dataset: Enter: ARIMA (2)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
  train = c(trend_train, rep(NA, length(trend_test))),
  test = c(rep(NA, length(trend_train)), trend_test),
  fitted = c(fit$fitted, rep(NA, length(trend_test))),
  preds = c(rep(NA, length(trend_train)), predictions),
  lower = c(rep(NA, length(trend_train)), lower),
  upper = c(rep(NA, length(trend_train)), upper))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.1)
```


Trend-only dataset: Enter: LSTM 
========================================================

&nbsp;

- Let's first show what we have to do for time series prediction with LSTM networks.

- We'll choose the <a href='https://keras.io/'>Keras</a> framework, and the R bindings provided by <a href='https://github.com/statsmaths/kerasR'>kerasR</a>.



Background: Data preparation for LSTM in Keras (1)
========================================================

Firstly, LSTMs work with a sliding window of input, so we need to provide the data (train and test) in "window form":

```{r}
# example given for training set, - do the same for test set
# length(trend_train)
lstm_num_timesteps <- 5
X_train <- t(sapply(1:(length(trend_train) - lstm_num_timesteps), function(x) trend_train[x:(x + lstm_num_timesteps - 1)]))
# dim(X_train)
X_train[1:5, ]

y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train)), function(x) trend_train[x])
y_train[1:5]
```

```{r, echo=FALSE}
X_test <- t(sapply(1:(length(trend_test) - lstm_num_timesteps), function(x) trend_test[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test)), function(x) trend_test[x])
```


Background: Data preparation for LSTM in Keras (2)
========================================================

Keras LSTMs expect the input array to be shaped as (no. samples, no. time steps, no. features)

```{r}
# example given for training set, - do the same for test set
# add 3rd dimension
dim(X_train)
X_train <- expand_dims(X_train, axis = 2)
dim(X_train)

# LSTM input shape: (samples, time steps, features)
num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]
c(num_samples, num_steps, num_features)
```

```{r, echo=FALSE}
X_test <- expand_dims(X_test, axis = 2)
```


Background: Keras - build the model
========================================================

&nbsp;


```{r}
model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
```


Background: Keras - fit the model!
========================================================

&nbsp;

```{r}
# not executed "live" ;-)
# keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```

We'll load the fitted model instead, and get the predictions.
========================================================

&nbsp;

```{r}
# we'll load the fitted model instead...
# keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)

model <- keras_load('trend_nodiff.h5')
pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
```



Hm. Whatever happened to predicting the test data?
========================================================


```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
  train = c(trend_train, rep(NA, length(trend_test))),
  test = c(rep(NA, length(trend_train)), trend_test),
  pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(trend_test))),
  pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


Anything we could have done to our data to make this work better?
========================================================

&nbsp;

- What if we had worked with the value differences, instead of the original values (learning from ARIMA & co.)?
- Or even the relative differences?
- What if we had scaled the data?


What if we work with differenced data?
========================================================

&nbsp;

```{r}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)
trend_test_diff

lstm_num_timesteps <- 4
```

```{r, echo=FALSE}
X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```


Get the predictions
========================================================


&nbsp;

```{r}
# we'll load the fitted model instead...
model <- keras_load('trend_diff.h5')
pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

# "undiff"
pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]
```


Differencing makes the difference...
========================================================



```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
   train = c(trend_train, rep(NA, length(trend_test))),
   test = c(rep(NA, length(trend_train)), trend_test),
   pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
   pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2))) 
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


Just for completeness, let's try relative differences as well
========================================================

&nbsp;


```{r}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)/trend_train[-length(trend_train)]
trend_test_diff <- diff(trend_test)/trend_test[-length(trend_test)]
trend_test_diff

lstm_num_timesteps <- 4
```

```{r, echo=FALSE}
X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```

Get the predictions
========================================================

```{r}
# we'll load the fitted model instead...
model <- keras_load('trend_reldiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
pred_train_undiff <- pred_train * trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)] + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test * trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)] + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]
```


Relative differences: results
========================================================

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
    train = c(trend_train, rep(NA, length(trend_test))),
    test = c(rep(NA, length(trend_train)), trend_test),
    pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
    pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


What if we difference AND normalize?
========================================================

&nbsp;


```{r}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 111:130 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

minval <- min(trend_train_diff)
maxval <- max(trend_train_diff)

normalize <- function(vec, min, max) {
  (vec-min) / (max-min)
}
denormalize <- function(vec,min,max) {
  vec * (max - min) + min
}

trend_train_diff <- normalize(trend_train_diff, minval, maxval)
trend_test_diff <- normalize(trend_test_diff, minval, maxval)

lstm_num_timesteps <- 4
```


```{r, echo=FALSE}
X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
```


Get the predictions
========================================================

```{r}
# we'll load the fitted model instead...
model <- keras_load('trend_diffnorm.h5')
pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

```


Difference and normalize: results
========================================================

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
    train = c(trend_train, rep(NA, length(trend_test))),
    test = c(rep(NA, length(trend_train)), trend_test),
    pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
    pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>ARIMA vs. LSTM, Round 2: Trend-only dataset, test data in-range</h1>




Would anything change if the test data were in the range already known by the model?
========================================================


```{r,fig.width=20,fig.height=10}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)
df <- data_frame(time_id = 1:120,
train = c(trend_train, rep(NA, length(trend_test))),
test = c(rep(NA, length(trend_train)), trend_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()

```


Trend-only dataset, test data in-range: Enter: ARIMA (1)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)
fit <- auto.arima(trend_train)
order <- arimaorder(fit)
# fit on the test set
refit <- Arima(trend_test, order=order[1:3], seasonal=order[4:6])
predictions <- refit$fitted
(test_rsme <- sqrt(sum((trend_test - predictions)^2))) 
```


Trend-only dataset, test data in-range: Enter: ARIMA (2)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:120,
  train = c(trend_train, rep(NA, length(trend_test))),
  test = c(rep(NA, length(trend_train)), trend_test),
  fitted = c(fit$fitted, rep(NA, length(trend_test))),
  preds = c(rep(NA, length(trend_train)), predictions))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) 
```


Trend-only dataset, test data in-range: Enter: LSTM (no differencing)
========================================================


```{r, echo=FALSE}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)

lstm_num_timesteps <- 5

X_train <- t(sapply(1:(length(trend_train) - lstm_num_timesteps), function(x) trend_train[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train)), function(x) trend_train[x])
X_test <- t(sapply(1:(length(trend_test) - lstm_num_timesteps), function(x) trend_test[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test)), function(x) trend_test[x])
# add 3rd dimension
X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)
# LSTM input shape: (samples, time steps, features)
num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')

#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('trend_i_nodiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps) - pred_test)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



Does it get even better with differencing?
========================================================


```{r, echo=FALSE}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

lstm_num_timesteps <- 4

X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('trend_i_diff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



And with relative differencing?
========================================================


```{r, echo=FALSE}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)/trend_train[-length(trend_train)]
trend_test_diff <- diff(trend_test)/trend_test[-length(trend_test)]

#!!
lstm_num_timesteps <- 4
#diffinv(trend_train_diff, xi=trend_train_start)

X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('trend_i_reldiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
pred_train_undiff <- pred_train * trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)] + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test * trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)] + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



Finally, what about both differencing and normalizing?
========================================================

```{r, echo=FALSE}
set.seed(7777)
trend_train <- 11:110 + rnorm(100, sd = 2)
trend_test <- 31:50 + rnorm(20, sd =2)

trend_train_start <- trend_train[1]
trend_test_start <- trend_test[1]

trend_train_diff <- diff(trend_train)
trend_test_diff <- diff(trend_test)

minval <- min(trend_train_diff)
maxval <- max(trend_train_diff)

normalize <- function(vec, min, max) {
  (vec-min) / (max-min)
}
denormalize <- function(vec,min,max) {
  vec * (max - min) + min
}

trend_train_diff <- normalize(trend_train_diff, minval, maxval)
trend_test_diff <- normalize(trend_test_diff, minval, maxval)

lstm_num_timesteps <- 4

X_train <- t(sapply(1:(length(trend_train_diff) - lstm_num_timesteps), function(x) trend_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(trend_train_diff)), function(x) trend_train_diff[x])
X_test <- t(sapply(1:(length(trend_test_diff) - lstm_num_timesteps), function(x) trend_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(trend_test_diff)), function(x) trend_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('trend_i_diffnorm.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + trend_train[(lstm_num_timesteps+1):(length(trend_train)-1)]
pred_test_undiff <- pred_test + trend_test[(lstm_num_timesteps+1):(length(trend_test)-1)]

df <- data_frame(time_id = 1:120,
                 train = c(trend_train, rep(NA, length(trend_test))),
                 test = c(rep(NA, length(trend_train)), trend_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(trend_test))),
                 pred_test = c(rep(NA, length(trend_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(trend_test,length(trend_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>ARIMA vs. LSTM, Round 3: seasonal-only dataset</h1>


Seasonal-only dataset 
========================================================

&nbsp;

```{r,fig.width=20,fig.height=10}
set.seed(7777)
seasonal_train <- rep(1:7, times = 13) + rnorm(91, sd=0.2)
seasonal_test <- rep(1:7, times = 3) + rnorm(21, sd=0.2)
df <- data_frame(time_id = 1:112,
train = c(seasonal_train, rep(NA, length(seasonal_test))),
test = c(rep(NA, length(seasonal_train)), seasonal_test))
df <- df %>% gather(key = 'train_test', value = 'value', -time_id)
ggplot(df, aes(x = time_id, y = value, color = train_test)) + geom_line()
```

Seasonal-only dataset: Enter: ARIMA (1)
========================================================

&nbsp;

```{r, fig.width=20,fig.height=10, error=TRUE}
set.seed(7777)
seasonal_train <- rep(1:7, times = 13) + rnorm(91, sd=0.2)
seasonal_test <- rep(1:7, times = 3) + rnorm(21, sd=0.2)
h <- 1
n <- length(seasonal_test) - h + 1
fit <- auto.arima(seasonal_train)
order <- arimaorder(fit)
refit <- Arima(seasonal_test, order=order[1:3], seasonal=order[4:6])
predictions <- refit$fitted
(test_rsme <- sqrt(sum((seasonal_test - predictions)^2))) 
```



Seasonal-only dataset: enter: ARIMA (2)
========================================================

```{r,fig.width=20,fig.height=10}
df <- data_frame(time_id = 1:112,
  train = c(seasonal_train, rep(NA, length(seasonal_test))),
  test = c(rep(NA, length(seasonal_train)), seasonal_test),
  fitted = c(fit$fitted, rep(NA, length(seasonal_test))),
  preds = c(rep(NA, length(seasonal_train)), predictions))
df <- df %>% gather(key = 'type', value = 'value', train:preds)
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type)) 
```


Seasonal-only dataset: Enter: LSTM (no differencing)
========================================================


```{r, echo=FALSE}
set.seed(7777)
seasonal_train <- rep(1:7, times = 13) + rnorm(91, sd=0.2)
seasonal_test <- rep(1:7, times = 3) + rnorm(21, sd=0.2)

lstm_num_timesteps <- 7

X_train <- t(sapply(1:(length(seasonal_train) - lstm_num_timesteps), function(x) seasonal_train[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(seasonal_train)), function(x) seasonal_train[x])
X_test <- t(sapply(1:(length(seasonal_test) - lstm_num_timesteps), function(x) seasonal_test[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(seasonal_test)), function(x) trend_test[x])
# add 3rd dimension
X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)
# LSTM input shape: (samples, time steps, features)
num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('seasonal_nodiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test),
                 pred_train = c(rep(NA, lstm_num_timesteps), pred_train, rep(NA, length(seasonal_test))),
                 pred_test = c(rep(NA, length(seasonal_train)), rep(NA, lstm_num_timesteps), pred_test))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(seasonal_test,length(seasonal_test) - lstm_num_timesteps) - pred_test)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



Does it get even better with differencing? (How could it ;-))
========================================================

```{r, echo=FALSE}
set.seed(7777)
seasonal_train <- rep(1:7, times = 13) + rnorm(91, sd=0.2)
seasonal_test <- rep(1:7, times = 3) + rnorm(21, sd=0.2)

seasonal_train_start <- seasonal_train[1]
seasonal_test_start <- seasonal_test[1]

seasonal_train_diff <- diff(seasonal_train)
seasonal_test_diff <- diff(seasonal_test)

lstm_num_timesteps <- 7

X_train <- t(sapply(1:(length(seasonal_train_diff) - lstm_num_timesteps), function(x) seasonal_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(seasonal_train_diff)), function(x) seasonal_train_diff[x])
X_test <- t(sapply(1:(length(seasonal_test_diff) - lstm_num_timesteps), function(x) seasonal_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(seasonal_test_diff)), function(x) seasonal_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('seasonal_diff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
pred_train_undiff <- pred_train + seasonal_train[(lstm_num_timesteps+1):(length(seasonal_train)-1)]
pred_test_undiff <- pred_test + seasonal_test[(lstm_num_timesteps+1):(length(seasonal_test)-1)]

df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(seasonal_test))),
                 pred_test = c(rep(NA, length(seasonal_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(seasonal_test,length(seasonal_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2))) 
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



Or with relative differencing (for completeness' sake)?
========================================================


```{r, echo=FALSE}
set.seed(7777)
seasonal_train <- rep(1:7, times = 13) + rnorm(91, sd=0.2)
seasonal_test <- rep(1:7, times = 3) + rnorm(21, sd=0.2)

seasonal_train_start <- seasonal_train[1]
seasonal_test_start <- seasonal_test[1]

seasonal_train_diff <- diff(seasonal_train)/seasonal_train[-length(seasonal_train)]
seasonal_test_diff <- diff(seasonal_test)/seasonal_test[-length(seasonal_test)]

lstm_num_timesteps <- 7

X_train <- t(sapply(1:(length(seasonal_train_diff) - lstm_num_timesteps), function(x) seasonal_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(seasonal_train_diff)), function(x) seasonal_train_diff[x])
X_test <- t(sapply(1:(length(seasonal_test_diff) - lstm_num_timesteps), function(x) seasonal_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(seasonal_test_diff)), function(x) seasonal_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('seasonal_reldiff.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)
pred_train_undiff <- pred_train * seasonal_train[(lstm_num_timesteps+1):(length(seasonal_train)-1)] + seasonal_train[(lstm_num_timesteps+1):(length(seasonal_train)-1)]
pred_test_undiff <- pred_test * seasonal_test[(lstm_num_timesteps+1):(length(seasonal_test)-1)] + seasonal_test[(lstm_num_timesteps+1):(length(seasonal_test)-1)]

df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(seasonal_test))),
                 pred_test = c(rep(NA, length(seasonal_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(seasonal_test,length(seasonal_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2))) 
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



Finally, what about both differencing and normalizing
========================================================


```{r, echo=FALSE}
set.seed(7777)
seasonal_train <- rep(1:7, times = 13) + rnorm(91, sd=0.2)
seasonal_test <- rep(1:7, times = 3) + rnorm(21, sd=0.2)

seasonal_train_start <- seasonal_train[1]
seasonal_test_start <- seasonal_test[1]

seasonal_train_diff <- diff(seasonal_train)
seasonal_test_diff <- diff(seasonal_test)

minval <- min(seasonal_train_diff)
maxval <- max(seasonal_train_diff)

normalize <- function(vec, min, max) {
  (vec-min) / (max-min)
}
denormalize <- function(vec,min,max) {
  vec * (max - min) + min
}

seasonal_train_diff <- normalize(seasonal_train_diff, minval, maxval)
seasonal_test_diff <- normalize(seasonal_test_diff, minval, maxval)

lstm_num_timesteps <- 7

X_train <- t(sapply(1:(length(seasonal_train_diff) - lstm_num_timesteps), function(x) seasonal_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(seasonal_train_diff)), function(x) seasonal_train_diff[x])
X_test <- t(sapply(1:(length(seasonal_test_diff) - lstm_num_timesteps), function(x) seasonal_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(seasonal_test_diff)), function(x) seasonal_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('seasonal_diffnorm.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + seasonal_train[(lstm_num_timesteps+1):(length(seasonal_train)-1)]
pred_test_undiff <- pred_test + seasonal_test[(lstm_num_timesteps+1):(length(seasonal_test)-1)]

df <- data_frame(time_id = 1:112,
                 train = c(seasonal_train, rep(NA, length(seasonal_test))),
                 test = c(rep(NA, length(seasonal_train)), seasonal_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(seasonal_test))),
                 pred_test = c(rep(NA, length(seasonal_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
(test_rsme <- sqrt(sum((tail(seasonal_test,length(seasonal_test) - lstm_num_timesteps - 1) - pred_test_undiff)^2)))
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```


Benchmark: Remarks and conclusions
========================================================

&nbsp;

- We've used a very basic setup for the LSTM
    - just one layer
    - no experiments with number of units, optimization routines, activation functions...
    - no use of dropout, regularization, weight decay...
    - not making use of Keras _stateful_ LSTM
    
- We've only compared both methods on a rolling forecast (not forecasting several periods into the future)



Aside (1): Stateful RNNs in Keras 
========================================================

&nbsp;


- With stateful RNNs, states computed for the samples in one batch will be reused as initial states for the _respective_ samples in the next batch
- "Makes sense" for a time series, as long as the data is reformatted or batch_size=1 is used
- Presupposes batches arriving in the same order in every epoch (set shuffle = False)
- Not currently implemented in KerasR - time for some Python...




========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Demo: Stateful LSTM in Keras</h1>



Aside (2): Multi-step-ahead forecasts in Keras 
========================================================

&nbsp;


- Multi-step-ahead forecasts using LSTM can be done in a number of ways:
    - build predictions on earlier predictions ("low end")
    - seq2seq architecture ("high end", not currently available out of the box in Keras)
    - using TimeDistributed layer (not currently implemented in KerasR)



========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Demo: Multi-step ahead forecasting in Keras using TimeDistributed</h1>


Finally...
========================================================


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>Forecasting internet traffic using LSTM</h1>



Here, again, is our time series...
========================================================

&nbsp;

```{r,fig.width=20,fig.height=10}
ggplot(traffic_df, aes(x = hour, y = bits)) + geom_line() + ggtitle("Internet traffic")

```



Forecasting internet traffic with LSTM
========================================================


We apply first-order differencing and normalize the data.

```{r}
traffic_train <- traffic_df$bits[1:800]
traffic_test <- traffic_df$bits[801:nrow(traffic_df)]

traffic_train_start <- traffic_train[1]
traffic_test_start <- traffic_test[1]

traffic_train_diff <- diff(traffic_train)
traffic_test_diff <- diff(traffic_test)

minval <- min(traffic_train_diff)
maxval <- max(traffic_train_diff)

normalize <- function(vec, min, max) {
  (vec-min) / (max-min)
}
denormalize <- function(vec,min,max) {
  vec * (max - min) + min
}

traffic_train_diff <- normalize(traffic_train_diff, minval, maxval)
traffic_test_diff <- normalize(traffic_test_diff, minval, maxval)

```

We choose 7*24=168 for the number of timesteps.

```{r,fig.width=20,fig.height=10}
lstm_num_timesteps <- 7*24
```

&nbsp;

And the results? ... Wow!
========================================================

&nbsp;

```{r, echo=FALSE}
X_train <- t(sapply(1:(length(traffic_train_diff) - lstm_num_timesteps), function(x) traffic_train_diff[x:(x + lstm_num_timesteps - 1)]))
y_train <- sapply((lstm_num_timesteps + 1):(length(traffic_train_diff)), function(x) traffic_train_diff[x])
X_test <- t(sapply(1:(length(traffic_test_diff) - lstm_num_timesteps), function(x) traffic_test_diff[x:(x + lstm_num_timesteps - 1)]))
y_test <- sapply((lstm_num_timesteps + 1):(length(traffic_test_diff)), function(x) traffic_test_diff[x])

X_train <- expand_dims(X_train, axis = 2)
X_test <- expand_dims(X_test, axis = 2)

num_samples <- dim(X_train)[1]
num_steps <- dim(X_train)[2]
num_features <- dim(X_train)[3]

model <- Sequential()
model$add(LSTM(units = 4, input_shape=c(num_steps, num_features)))
model$add(Dense(1))
keras_compile(model, loss='mean_squared_error', optimizer='adam')
#keras_fit(model, X_train, y_train, batch_size = 1, epochs = 500, verbose = 1)
model <- keras_load('traffic_diffnorm.h5')

pred_train <- keras_predict(model, X_train, batch_size = 1)
pred_test <-keras_predict(model, X_test, batch_size = 1)

pred_train <- denormalize(pred_train, minval, maxval)
pred_test <- denormalize(pred_test, minval, maxval)

pred_train_undiff <- pred_train + traffic_train[(lstm_num_timesteps+1):(length(traffic_train)-1)]
pred_test_undiff <- pred_test + traffic_test[(lstm_num_timesteps+1):(length(traffic_test)-1)]

df <- data_frame(time_id = 1:1231,
                 train = c(traffic_train, rep(NA, length(traffic_test))),
                 test = c(rep(NA, length(traffic_train)), traffic_test),
                 pred_train = c(rep(NA, lstm_num_timesteps+1), pred_train_undiff, rep(NA, length(traffic_test))),
                 pred_test = c(rep(NA, length(traffic_train)), rep(NA, lstm_num_timesteps+1), pred_test_undiff))
df <- df %>% gather(key = 'type', value = 'value', train:pred_test)
```

```{r,fig.width=20,fig.height=10}
ggplot(df, aes(x = time_id, y = value)) + geom_line(aes(color = type))
```



The end?
========================================================

&nbsp;

- This is more like a beginning
- Lots of things to explore...
    - experiment with parameters, architectures...
    - experiment with different datasets...
- Have fun!

&nbsp;

Thanks for your attention!!
